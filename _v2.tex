%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required

%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{alltt}

\usepackage{amsmath} % Required for \DeclareMathOperator*
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{amssymb,amsfonts}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fancyvrb}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
%\usepackage{lmodern}
\usepackage{url}
% \usepackage[hidelinks]{hyperref}
\usepackage{adjustbox}

\usepackage{wrapfig} 
\usepackage{changepage} 
\usepackage{framed}
\usepackage{enumitem}
\usepackage{cite}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}

\definecolor{niceblue}{HTML}{0000FF}

\newcommand{\BLUE}{\color{niceblue}}
\newcommand{\BLACK}{\color{black}}

 \usepackage[hidelinks]{hyperref}

\newcommand{\there}[1]{%
  \hyperlink{resp:#1}{%
    %\textcolor{gray}{➜}~%
    \fcolorbox{black}{black!15}{%
      \bfseries\scriptsize #1%
    }~on page \pageref{resp:#1}%
  }%
}


\newcommand{\here}[1]{%
  \hypertarget{resp:#1}{}% create the target anchor
  \fcolorbox{black}{black!15}{%
    \label{resp:#1}%
    \bfseries\scriptsize  {#1}%
  }%
}



\definecolor{lightblue}{rgb}{0.85,0.9,1}
\definecolor{blue}{rgb}{0.5,0.75,1}
\definecolor{darkblue}{rgb}{0,0.5,1}

\definecolor{lightred}{rgb}{1,0.85,0.85}
\definecolor{red}{rgb}{1,0.5,0.5}
\definecolor{darkred}{rgb}{1,0,0}

\definecolor{codeblue}{rgb}{0.1,0.1,0.7}
\definecolor{codegreen}{rgb}{0.1,0.6,0.1}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{verylightgray}{rgb}{0.95, 0.95, 0.95}

\newcommand{\coloredcell}[2]{\cellcolor{#1}{\rule{0pt}{0.5ex}\scriptsize #2}}

\usepackage{bm}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\title{From Brittle to Robust: \\Improving LLM Annotations for SE Optimization 
%If Not One Then Many: Ensemble of Few Shot Learners for Uncommon SE Optimization Problems %\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}


%\titlerunning{Short form of title}        % if too long for running head

\author{Lohith Senthilkumar         \and
        Tim Menzies %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{All authors are from Computer Science, North Carolina State University
   Oval Dr,  Raleigh, NC 27606
\\\email{panjal@ncsu.edu, timm@ieee.org}}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

 
\maketitle

\begin{abstract}
Software analytics often builds 
from labeled data.
Labeling   can be slow, error prone, and expensive.
When human expertise is scarce,
SE researchers sometimes
ask large language models (LLMs) for the   missing labels.

While this has been successful in some domains,
recent results show that LLM-based  labeling has blind spots.
Specifically, their labeling is not effective for  higher dimensional multi-objective problems.


To address this task, we propose a novel LLM prompting strategy called SynthCore. When one opinion fails,  SynthCore's   combines multiple separated opinions generated by LLMs (with no knowledge of each others' answers) into
an ensemble of few-shot learners. 
Simpler than other strategies (e.g. chain-of-thought, multi-agent-debate, etc)   SynthCore   aggregates results from    multiple single prompt sessions (with no crossover between them). 

  SynthCore has been tested on 49 SE multi-objective optimization tasks,
 handling tasks as diverse as software project management, Makefile configuration, and hyperparameter optimization.
SynthCore's
ensemble   found optimizations
that are better than state-of-the-art alternative approaches  (Gaussian Process Models, Tree of Parzen Estimators,
active learners in both exploration and exploitation mode). 
Importantly, these optimizations were made using data labeled by LLMs, without any human opinions.

From these experiments, we conclude that ensembles of few shot learners can successfully annotate high dimensional multi-objective tasks. Further, we speculate that other successful few-shot prompting results could be quickly and easily enhanced  using SynthCore's ensemble approach.

To support open science, all our data and scripts are available  at \url{https://github.com/lohithsowmiyan/lazy-llm/tree/clusters}.


 

\keywords{Active Learning \and Large Language Models \and Multi-Objective Optimization}
% \PACS{PACS code1 \and PACS code2 \and more}
\end{abstract}





\section{Introduction}
% Multi-objective optimization is a process for solving problems that involve several, often conflicting, objectives simultaneously. Many problems in SE involve multiple competing objectives, such as Minimizing test suite execution time and maximizing fault detection in test case prioritization \cite{yoo2012regression}, balancing performance, scalability and maintanence in software architecture recovery by \cite{ramirez2016comparative}. 

% Many SE researchers have recommended the use of Evolutionary Algorithms (EA) \cite{harman2010software,hanne2005multiobjective,grunske2006identifying,wang2010multi} to find the Pareto Optimal Front. However, a major drawback of using the above methods is that they incur high labeling costs (few hundred to a thousand). Given the paucity of subject matter experts (SMEs) within an organization it is more often not a recommended practice to use these algorithms in domains with strict budget constraints. Recent works preferred using Surrogate Models over EAs \cite{} to reduce the labeling costs from (50 to 200) labels on average. Sequential Model Based Optimization methods like FLASH \cite{Nair} and \cite{} found success in reducing the budget by exploring uncertain regions using the previously sampled points. Usage of adaptive methods to balance exploration and exploitation of sampled regions \cite{} has been one of most popular approaches in finding optimal configurations.

% \cite{krauth2016autogp} show that higher the complexity of dataset more the number of samples taken by Bayesian methods to explore different regions, thereby demanding for more labeling. The above findings fail to consider the difference between Software Engineering and Non Software Engineering Data. \cite{} report that SE data have lesser intrinsic dimensions compared to Non SE datasets. This begs the question 'is there an easier/faster method for exploration in SE data'?. \cite{senthilkumar2024can} used large language models (LLMs) to to perform fast initial explorations "warm-starts" followed by subsequent exploitations, however the method failed to scale up for complex and high dimensional datasets. (NEED TO ADD MORE)


% Given the linearity in SE data and using LLM to perform quicker explorations we propose a unique active learning method to synthesize better configurations with a series of LLM based warm starts in multiple directions. Accordingly, we explore SynthCore for the 49 multi-objective SE optimization tasks \cite{} shown in Table 1. To structure this investigation, we ask:
% \begin{itemize}
%     \item RQ1: Does uncertainty based optimizers scale up well accross datasets?
%     \item RQ2: Are Large Language Models useful black box optimizers?
%     \item RQ3: Can Large Language Models simplify exploration?
% \end{itemize}

% In summary the contributions of the paper involve
% \begin{itemize}
%     \item A novel method for using Large Language Model for SE active learning
%     \item Strengths and Limitations of using LLMs over traditional over traditional methods
%     \item A python package to reproduce our results
% \end{itemize}


% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{LaTeX_DL_468198_240419/Blank diagram-54.jpeg}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{LaTeX_DL_468198_240419/Unknown-8.png}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}


% \begin{table}[!t]
% \caption{Data sets  used in this paper. $x/y$ shows the number of independent $x$ vales and dependent $y$ values. The last column show the heuristic categorization of   data sets by
% Di Fiore et al.~\cite{difiore2024}  (data sets with  less than 6 or 11 $x$ columns are 
% {\em low} or 
% {\em medium} complexity while other data sets are
% {\em high} complexity).  
% For further notes on this data, see \S\ref{data}.}\label{dataset}
 
% {\scriptsize
% \begin{tabular}{p{1.5cm}|crcl}  
% \textbf{Groups} & \textbf{File Name} & |rows| & \textbf{$x$ / $y$} & \textbf{Dims.} \\ \hline
% \multirow{32}{*}{Software Config} 
%     & SS-A    & 864    & 3/2  & low      \\
%     & SS-B    & 972    & 3/2  & low      \\
%     & SS-C    & 1080   & 3/2  & low      \\
%     & SS-D    & 2736   & 3/2  & low      \\
%     & SS-E    & 3008   & 3/2  & low      \\
%     & SS-F    & 3839   & 3/2  & low      \\
%     & SS-G    & 5184   & 3/2  & low      \\
%     & SS-H    & 4608   & 4/2  & low      \\
%     & SS-I    & 6840   & 5/2  & low      \\
%     & SS-J    & 65536  & 6/2  & medium   \\
%     & SS-K    & 86058  & 6/2  & medium   \\
%     & SS-L    & 1023   & 11/2 & low      \\
%     & SS-M    & 864    & 17/3 & high     \\
%     & SS-N & 86058 & 18/2 & high \\
%     & SS-O    & 972    & 11/2 & high     \\
%     & SS-P    & 1023   & 11/2 & high     \\
%     & SS-Q    & 2736   & 13/3 & high     \\
%     & SS-R    & 3008   & 14/2 & high     \\
%     & SS-S    & 3840   & 6/2  & medium   \\
%     & SS-T    & 5184   & 12/2 & high     \\
%     & SS-U    & 4608   & 21/2 & high     \\
%     & SS-V    & 6840   & 16/2 & high     \\
%     & SS-W    & 65536  & 16/2 & high     \\
%     & SS-X    & 86058  & 11/3 & high     \\
%     & Apache AllMeasurements & 192 & 9/1 & medium \\
%     & SQL AllMeasurements    & 4653 & 38/1 & high \\
%     & X264 AllMeasurements   & 1152 & 16/1 & high \\
%     & rs-6d-c3 obj1          & 3840 & 6/1  & medium \\
%     & rs-6d-c3 obj2          & 3840 & 6/1  & medium \\
%     & sol-6d-c2-ob j1        & 2866 & 6/1  & medium \\
%     & wc-6d-c1-ob j1         & 2880 & 6/1  & medium \\
%     & wc+sol-3d-c4-ob j1     & 196  & 3/1  & low \\
%     & wc+rs-3d-c4-obj1       & 196 & 3/1 & low\\
%     & wc+wc-3d-c4-ob j1      & 196  & 3/1  & low \\
%     \hline
% \multirow{10}{*}{Software Process Modeling} 
%     & pom3a          & 500    & 9/3 & medium \\
%     & pom3b          & 500    & 9/3 & medium \\
%     & pom3c          & 500    & 9/3 & medium \\
%     & pom3d          & 500    & 9/3 & medium \\
%     & xomo\_flight   & 10000  & 23/4 & high \\
%     & xomo\_ground   & 10000  & 23/4 & high \\
%     & xomo\_osp      & 10000  & 23/4 & high \\
%     & xomo\_osp2     & 10000  & 23/4 & high \\
%     & coc1000        & 1000   & 17/5 & high \\
%     & nasa93dem      & 93     & 22/4 & high \\
%     \hline
% Project  Health 
%     & healthCloseIsses12mths0001-hard & 10000 & 5/1 & low \\
%     & healthCloseIsses12mths0011-easy.csv & 10000 & 5/1 & low \\
%     \hline
% \multirow{3}{*}{Miscellaneous} 
%     & auto93         & 398    & 5/3 & low \\
%     & Wine\_quality  & 1599   & 10/2 & medium \\
%     & HSMGP num      & 3456   & 14/1 & high  
% \end{tabular}}
% \end{table}

Software Engineering (SE) data is often costly and labor-intensive to obtain, typically requiring careful curation by subject matter experts (SMEs) to ensure both accuracy and relevance~\cite{lustosa2024learning}. Yet, as demand grows for large-scale, high-quality datasets across a range of SE tasks, researchers have increasingly turned to automation to reduce annotation costs~\cite{ahmed2024can}. Among these, the use of Large Language Models (LLMs) as automated oracles for labeling has emerged as a compelling option~\cite{wan2024tnt}.
% With their capacity for generalization across domains and their ability to simulate human-like reasoning, LLMs appear well-positioned to streamline SE data collection pipelines.

However, LLM-based annotation is not without challenges. In a recent empirical study presented at the  MSR 2025
conference, Ahmed et al.~\cite{ahmed2024can} systematically evaluated LLM performance on five SE labeling tasks. Their findings reveal a nuanced picture: while LLMs can at times match human-level agreement on straightforward labeling tasks, they frequently fall short on higher dimensional or ambiguous problems. Key issues include hallucinated outputs, unjustified confidence in incorrect answers, and brittleness when applied to unfamiliar or out-of-distribution inputs. Crucially, Ahmed et al. warn that even when LLMs exhibit high inter-annotator or human-model agreement, this is not a reliable proxy for ground-truth correctness-particularly in domains that demand deep software-specific reasoning. They argue that LLMs should be used as conditional collaborators rather than drop-in replacements for human annotators: 

\begin{quote}
{\em ``We do not claim that LLMs can universally replace human annotators; instead, our findings suggest that they may be viable complements—especially when \underline{\bf used carefully} and \underline{\bf selectively.}.}~\cite{ahmed2024can}
\end{quote}
\BLUE
\here{R3c} This paper reports a {\em careful} and {\em selective} use of LLMs for annotating examples used in {\em multi-objective optimization} tasks. By multi-objective optimization, we refer to problems where the goal is not to optimize a single output but to simultaneously balance several (often competing) objectives. This differs from the tasks studied by Ahmed et al., where the output space is typically single-dimensional. In contrast, our optimization setting requires annotations that guide trade-offs across multiple domain constraints. For instance, when evaluating software design options, practitioners may need to find alternatives that allow them to:
\bi
\item deliver the {\em most} features,
\item in the {\em least} time,
\item at the {\em lowest} cost,
\item with the {\em fewest} defects.
\ei
Other examples of multi-objective problems in SE include:
\begin{itemize}
\item
{\em Green engineering} ensures faster responses that use less energy;
\item
and {\em Hyperparameter optimization} finds learner parameters that minimize false alarms and maximize recall;
\item
And any 
{\em Configuration task}
such as  selecting
magic control variables within a Makefile.
\end{itemize}
Modern software urgently
needs better tools for automatic configuration. We say this since all to often, software is deployed with suboptimal configurations~\cite{Krishna:2016}.  For example, \textsc{Storm}'s defaults yield $480\times$ performance degradation vs. optimal parameters~\cite{DBLP:conf/mascots/JamshidiC16}. Such poor performance is hardly surprising since industrial optimization faces major obstacles:
\begin{itemize}
    \item Configuration spaces explode exponentially (in \textsc{7z}: 14 parameters = one million configurations).
    \item Performance landscapes are rugged and sparse~\cite{DBLP:journals/tosem/GongC25,lustosa2024learning,chen2026promisetune}, creating local optimal traps.
    \end{itemize}
    Evaluating a wide range of different configurations
    can be very costly; e.g. \textsc{x264}'s 11 parameters need $1,536$ hours to explore~\cite{DBLP:conf/wosp/ValovPGFC17}, limiting budgets to dozens of evaluations~\cite{DBLP:journals/tse/Nair0MSA20,DBLP:conf/icse/0003XC021}.
 {\em Active learning} can reduce that cost by evaluating only the most informative examples. But the success of any optimizer—including active learning—can depend heavily on the availability of reasonable initial examples. In active learning, the learner selectively queries or acquires labels, but its ability to do so effectively often hinges on a high-quality initial set of annotated samples. These initial samples, commonly referred to as ``warm starts,'' serve as the starting point from which the learner begins exploring the space of candidate solutions.
 
Generating such warm starts has proven to be challenging. Recent work~\cite{senthilkumar2024can} shows that LLMs often struggle to produce high-quality initial examples, particularly in domains that are unfamiliar or underrepresented in their training corpora. We have previously argued that this limitation arises from insufficient training data coverage:
\bi
\item LLMs excel when tasks are frequent, well-represented, and structurally consistent with patterns in their training data.
\item But when confronted with rare, highly specialized, or high-dimensional optimization problems, their outputs tend to be shallow, generic, or systematically misinformed.
\item For details of these findings, see 
Table~\ref{lo0},~\ref{med0}~\ref{hi0}.
\ei
\BLACK
To overcome this limitation, this paper proposes a novel prompting strategy called SynthCore.
Rather than relying on a single monolithic answer from the LLM, SynthCore prompts the model multiple times—each time under slightly different contexts or with randomized seeds—and then synthesizes the resulting outputs into a composite candidate set. 
 Simpler than other strategies (e.g.
chain-of-thought, multi-agent debate, etc) \cite{yuan2025ui2html,yang2024chain,du2024multi} SynthCore aggregates results from multiple
single prompt sessions (with no crossover between them).\BLUE \here{R2a-1} It also differs substantially from self-consistency prompting strategies that rely on a voting based mechanism to select the majority output while decoding. \cite{ahmed2023better}. Moreover, prior work on seed generation \cite{xia2024fuzz4all} has shown that varying few-shot examples encourages language models to produce more diverse seeds—an insight that we regard as central to our method. \BLACK By aggregating diverse perspectives from the LLM, SynthCore avoids the pitfalls of overconfident single-shot reasoning and improves the chances of capturing useful problem structure.

We evaluated SynthCore on 49 multi-objective SE optimization tasks spanning domains such as project planning, Makefile tuning, and hyperparameter selection. Across the board, SynthCore’s ensemble-based strategy delivered superior performance compared to state-of-the-art techniques, including Gaussian Process Models, Trees of Parzen Estimators, and both exploitative and exploratory active learners. Notably, all labels used in SynthCore’s experiments were generated solely by LLMs—no human annotations were included. These results demonstrate that,
\begin{quote}
{\em
While individual LLM responses may be unreliable, {\bf a carefully curated ensemble of few-shot learners} can collectively overcome their shortcomings.}
\end{quote}
Thus, we conclude that with the right orchestration, LLMs can move from brittle annotators to robust contributors in SE automation pipelines.

To introduce SynthCore, this paper explores two research questions 


\BLUE
\here{R3e}
\begin{itemize}

    \item{\textbf{RQ1:} {\em While standard few-shot learning often struggles in SE Active Learning, does ensemble-based few-shot learning overcome these limitations and achieve better performance?} This is the core question of this research.}
    \item {\textbf{RQ2:} {\em Does the effectiveness of ensemble-based few-shot learners hold when dealing with high-dimensional SE data?} This work was motivated by a prior study that reported
    LLMs failed   on
 higher dimensions.} Therefore,
    in our results, we must pay particular
    attention to this kind of data.
    
    \end{itemize}
As seen in the results of this paper: 
\bi
\item
{\bf RQ1} Yes, the results demonstrates the superiority of ensemble LLM Learners  in SE multi-objective optimization tasks.
\item

{\bf RQ2} Yes, the results shows that ensembles of few shot learners solve the problem reported in prior work.  

\ei

 \BLACK
The contributions in this paper include:
\begin{enumerate}
    \item We propose an novel prompting tactic: ensembles of LLM few-shot learners.
\item We test  the effectiveness of this approach for 49 SE datasets.
\item We compare this approach with  alternative methods for 49 datasets.
\item We offer  a reproduction package with all our data and scripts\footnote{  \url{https://github.com/lohithsowmiyan/lazy-llm/tree/clusters}}.
\end{enumerate}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{promting.png}
%     \caption{Various prompting strategies and our ensemble prompting method}
%     \label{fig:placeholder}
% \end{figure}
Based on this work,
we speculate
that other successful few-short prompting results could be quickly and easily
enhanced using SynthCore’s ensemble approach.
This is not a proven contribution but the results of this paper
suggest it would be a useful 
future research direction for prompt engineering and software analytics.

  \BLUE 
 
  \subsection{Digression}
  \here{R0a} Before beginning,
 it is important to clearly define the scope of this work. This paper does not investigate which specific LLM architecture is optimal for these optimization tasks. While we have preliminary hypotheses on that matter, a comprehensive exploration of model-specific performance remains a critical direction for future research.
 
 What this paper offers is an important finding regarding  LLMs and optimization: contrary to recent pessimistic results~\cite{senthilkumar2024can}, LLMs are highly effective for optimization problems, even with high-dimensional data. Our results demonstrate the definitive superiority of LLMs over long-standing, state-of-the-art symbolic methods for this task. This is an important contribution to the Software Engineering (SE) literature, which currently suffers from insufficient comparative evaluation of LLMs. For instance, a recent systematic review of 229 SE papers using LLMs found that only $13/229 \approx 5\%$ compared LLMs to other approaches~\cite{Hou24}. 
 It is important to explore this gap since the prevailing hype often obscures the fact that LLMs frequently do not outperform more established methods in many domains~\cite{grinsztajn2022why,somvanshi2024survey,Tawosi23,majumder2018500+,Fu17,johnson2024ai}. What we can offer here  is good news since we can offer  robust evidence that LLMs offer a significant, practical performance advantage over conventional methods in the domain of SE optimization.
 

  
  
\section{Literature Review}
\subsection{Annotations in Software Engineering}
Manual annotations are vital for data-driven Software Engineering (SE)
tasks, influencing empirical findings \cite{easterbrook2008selecting}. Annotated
datasets underpin research in defect prediction, vulnerability detection,
sentiment analysis, and static analysis validation
\cite{menzies2025retrospective}. Redundancy, using multiple annotators,
ensures reliability and mitigates bias \cite{morse2002verification}.
Examples include 11 annotators for Jupyter Notebook inconsistency
\cite{patra2022nalin} and 3 annotators (from 11) for Java method
similarity \cite{kamp2019sesame}. Manual labeling is costly. Generic
automated tools often fail in SE due to specialized terminology
\cite{7962370}, leading to the development of domain-specific models like SENTI-STRENGTH
(78\% accuracy, 85\% recall) \cite{7962370}.

Annotation quality is crucial but faces challenges:
\begin{itemize}
\item \textbf{Heuristic Labeling Pitfalls}: Lexical patterns (e.g., "bug",
"fix") for defect prediction yield inconsistent ground truth without
rigorous validation \cite{vasilescu2015quality}.
\item \textbf{Validation Discrepancies}: Manual errors are common; 90\% of
false positives in one technical debt analysis were labeling errors
\cite{9226105}.
\end{itemize}
Manual annotation can be very slow.
\begin{itemize}
\item 
Unlike
medical labs where automated agents   label  based
on clearly defined definitions~\cite{krasowski2014autoverification}, SE annotation is far more subjective.  
\item
Valerdi reports studies with panels of human experts
in software effort estimation. Those panels
could handle only 13.3
examples/hour \cite{valerdi2010heuristics}.
\item
Lustosa reports studies with humans were complex management ranking
peaked at only 15-20 judgments/hour \cite{lustosa2024learning}. 
\item
In prior work with industrial subject matter experts,
we found expert availability
was very limited (rarely $>3$ hours/week) \cite{menzies1992expert}. Also, when we could negotiate
for more access, those longer
sessions decreased label quality \cite{EASTERBYSMITH19803}
(due to cognitive fatigue).
\end{itemize}
\subsection{Can LLMs Replace Manual Annotators in SE?}
In theory, automated annotation methods address manual limitations
\cite{dollmann2016and}. Large Language Models (LLMs) are promising
due to their pre-training on vast corpora, which allows them to adapt to
domain-specific contexts without extensive local training. LLMs are
used in code generation \cite{svyatkovskiy2020intellicode} and testing
\cite{deng2023large}.

Ahmed et al. \cite{ahmed2024can} explored if LLMs can replace manual
annotators on 10 SE tasks (e.g., code summarization, semantic similarity,
static analysis labeling). LLMs achieved \textbf{moderate to substantial
agreement} with humans in \textbf{6 out of 10 tasks} (e.g., non-functional
requirements) \cite{ahmed2024can}. However, performance declined
significantly in context-intensive tasks (e.g., static analysis warnings),
showing low model-model agreement, suggesting LLMs cannot fully
replace humans, especially for nuanced tasks.


Senthilkumar et al. \cite{senthilkumar2024can} reached similar
conclusions about the limits of LLMs for labeling.
That study
used LLM annotations to generate \textit{warm-start} samples for
\textit{active learning} in multi-objective SE tasks.
They found LLMs   useful for low-dimensional probems, but not
high-dimensional, multi-objective optimization problems
(this conclusion persisted even after applying feature
synthesis to artifically  reduce dimensionality). 

% \textbf{Key Concepts:}
% \begin{itemize}
% \item \textbf{Multi-objective reasoning} evaluates solutions across
% conflicting goals (e.g., maximize accuracy, minimize cost) \cite{sayyad2013scalable}.
% \item \textbf{Active learning} iteratively selects the most informative
% unlabeled examples to query, reducing labeling effort \cite{settles2012active}.
% \item \textbf{Warm-start samples} are initial, diverse inputs for
% optimization algorithms \cite{settles2012active}.
% \end{itemize}
% Active learning requires an initial warm-start (random, diversity
% sampling, or expert input). This paper explores LLMs as expert
% surrogates.

Senthilkumar et al. used the MOOT repository (\cite{Moot:2025}, see Table~\ref{dataset}), a collection
of SE multi-objective optimization tasks with $x$ independent variables
(3 to 38) and $y$ goals (1 to 5). Datasets were categorized by
$x$-dimensionality: \textit{Low} ($|x| < 6$), \textit{Medium} ($6 \le |x| \le 11$),
and \textit{High} ($|x| > 11$) \cite{difiore2024}. They found: 

 
\begin{itemize}
\item For \textbf{Low-dimensional tasks}: LLM warm-starts matched or
surpassed symbolic traditional  (Gaussian Process and TPE methods.~\cite{bergstra2011algorithms}).
\item For \textbf{Medium-dimensional problems}: same result;
\item For \textbf{High-dimensional problems}: LLM warm-starts underperformed,
often falling below baselines \cite{senthilkumar2024can}.
\end{itemize}
They concluded that  LLMs struggle to generalize to complex, high-dimensional
optimization, limiting their utility beyond trivial problems. That result,
which is very negative for LLM-based research,
motivates us to explore alternate methods (hence this paper).  


\begin{table}
\caption{Data used in this study.
As per  Di Fiore et al.~\cite{difiore2024},
data is labeled low, medium, or high dimensional
based on the number of $x$ independent goals. 
 $|x| < 6$ means "low" ;
 $6 \le |x| \le 11$ means "medium" and  $|x| > 11$  means "high". For more details on this data, see \S\ref{data}.}\label{dataset}
 
\begin{center}
{\footnotesize
\begin{tabular}{p{1.7cm}|lrrl}  
\textbf{Groups} & \textbf{File Name} & $|$rows$|$ & \textbf{$|x|$ / $|y|$} & \textbf{Dimsionality} \\ \hline
\multirow{32}{*}{Configuration} 
    & SS-A    & 864    & 3/2  & low      \\
    & SS-B    & 972    & 3/2  & low      \\
    & SS-C    & 1080   & 3/2  & low      \\
    & SS-D    & 2736   & 3/2  & low      \\
    & SS-E    & 3008   & 3/2  & low      \\
    & SS-F    & 3839   & 3/2  & low      \\
    & SS-G    & 5184   & 3/2  & low      \\
    & SS-H    & 4608   & 4/2  & low      \\
    & SS-I    & 6840   & 5/2  & low      \\
    & SS-J    & 65536  & 6/2  & medium   \\
    & SS-K    & 86058  & 6/2  & medium   \\
    & SS-L    & 1023   & 11/2 & low      \\
    & SS-M    & 864    & 17/3 & high     \\
    & SS-N & 86058 & 18/2 & high \\
    & SS-O    & 972    & 11/2 & high     \\
    & SS-P    & 1023   & 11/2 & high     \\
    & SS-Q    & 2736   & 13/3 & high     \\
    & SS-R    & 3008   & 14/2 & high     \\
    & SS-S    & 3840   & 6/2  & medium   \\
    & SS-T    & 5184   & 12/2 & high     \\
    & SS-U    & 4608   & 21/2 & high     \\
    & SS-V    & 6840   & 16/2 & high     \\
    & SS-W    & 65536  & 16/2 & high     \\
    & SS-X    & 86058  & 11/3 & high     \\
    & Apache AllMeasurements & 192 & 9/1 & medium \\
    & SQL AllMeasurements    & 4653 & 38/1 & high \\
    & X264 AllMeasurements   & 1152 & 16/1 & high \\
    & rs-6d-c3 obj1          & 3840 & 6/1  & medium \\
    & rs-6d-c3 obj2          & 3840 & 6/1  & medium \\
    & sol-6d-c2-ob j1        & 2866 & 6/1  & medium \\
    & wc-6d-c1-ob j1         & 2880 & 6/1  & medium \\
    & wc+sol-3d-c4-ob j1     & 196  & 3/1  & low \\
    & wc+rs-3d-c4-obj1       & 196 & 3/1 & low\\
    & wc+wc-3d-c4-ob j1      & 196  & 3/1  & low \\
    \hline
\multirow{10}{*}{  Process  } 
    & pom3a          & 500    & 9/3 & medium \\
    & pom3b          & 500    & 9/3 & medium \\
    & pom3c          & 500    & 9/3 & medium \\
    & pom3d          & 500    & 9/3 & medium \\
    & xomo\_flight   & 10000  & 23/4 & high \\
    & xomo\_ground   & 10000  & 23/4 & high \\
    & xomo\_osp      & 10000  & 23/4 & high \\
    & xomo\_osp2     & 10000  & 23/4 & high \\
    & coc1000        & 1000   & 17/5 & high \\
    & nasa93dem      & 93     & 22/4 & high \\
    \hline
Project~Health 
    & healthCloseIsses12mths0001-hard & 10000 & 5/1 & low \\
    & healthCloseIsses12mths0011-easy.csv & 10000 & 5/1 & low \\
    \hline
\multirow{3}{*}{Miscellaneous} 
    & auto93         & 398    & 5/3 & low \\
    & Wine\_quality  & 1599   & 10/2 & medium \\
    & HSMGP num      & 3456   & 14/1 & high  
\end{tabular}}
\end{center}
\end{table}



\subsection{Insight into why LLMs fail}
LLM success depends on training data diversity. In SE, data can be
sparse or biased, leading LLMs to inadequate initial hypotheses
\cite{bender2021dangers}. To compensate, \textit{diversity of inference}
uses techniques like prompt ensembles to explore a broader spectrum of
candidate solutions during inference \cite{wang2023selfConsistency}.

%\subsection{Can ``Ensemble Learning'' Repair Failing LLMs?}
Given the last paragraph, it seems  appropriate
to explore
\textit{ensemble learning}~\cite{dietterich2000ensemble} where conclusions
are formed from multiple experts, each working
slightly different versions of the data.
Ensemble learning is know to   boost robustness and predictive accuracy,
especially in few-shot, data-scarce settings \cite{dietterich2000ensemble}.
For example,
ensembles (e.g., Random Forest \cite{breiman2001randomForests}) outperform
single models by resisting noise and generalizing better on
high-dimensional data. 
% Techniques like "self-consistency" (sampling
% multiple reasoning paths) \cite{wang2023selfConsistency} and promoting
% diversity in prompting \cite{lau2024dipper} show promise in applying
% ensemble principles to LLMs for few-shot tasks.

% The conjecture is that ensembles can aid high-dimensional multi-objective
% annotation via:
% \begin{itemize}
% \item \textit{Diversity in Ensembles for Generalization:} Varied prompts
% foster improved generalization \cite{lau2024dipper}.
% \item \textit{Aggregation of Multiple Reasoning Paths:} Self-consistency
% improves performance on complex tasks \cite{wang2023selfConsistency}.
% \end{itemize}

\BLUE

\subsection{Prompting Strategies}
\here{R2a-2}
A wide range of prompting strategies have been explored in recent literature~\cite{ahmed2022few,le2023log,nashid2023retrieval,li2025structured,yang2024chain,ahmed2023better,shinn2023reflexion}. These strategies vary depending on the nature of the problem space, but several have emerged as particularly influential, including \emph{few-shot prompting}, \emph{chain-of-thought prompting}, \emph{self-consistency}, and \emph{self-reflection} approaches. Our own variant of prompting is described in 
\S\ref{prompt}.


Few-shot prompting~\cite{ahmed2022few,le2023log,nashid2023retrieval} is the standard practice of providing a small number of labeled examples (typically $n = 1$ to $4$--$5$) within the prompt. This helps large language models generalize to previously unseen tasks. Many advanced prompting techniques can be viewed as extensions or modifications of the few-shot paradigm.


Chain-of-thought (CoT) prompting~\cite{li2025structured,yang2024chain} extends few-shot prompting by including step-by-step explanations or reasoning traces along with each example. This encourages the model to follow explicit reasoning structures when solving unseen tasks, often improving interpretability and output quality. However, CoT has a notable drawback: if the provided reasoning path is incorrect or biased, the model may confidently follow and amplify flawed logic.


Self-consistency prompting~\cite{ahmed2023better} addresses the limitations of standard CoT. Instead of generating a single explanation--answer pair, the model generates multiple reasoning paths during decoding. These paths are then aggregated, typically via a voting mechanism, to select the most reliable final answer. This reduces sensitivity to any single, potentially incorrect chain of reasoning.


Self-reflection prompting~\cite{shinn2023reflexion} represents a distinct strategy in which the model iteratively refines its responses. In this setting, the model evaluates its previous outputs across a sequence of interactions (question $\rightarrow$ answer $\rightarrow$ critique $\rightarrow$ revised answer), enabling incremental improvement across iterations.

\begin{table}[!t]
\centering
\BLUE
\footnotesize
\begin{tabular}{|p{2cm}|p{4cm}|p{4cm}|}
\hline
\textbf{Feature} & \textbf{Self-Consistency} ~\cite{ahmed2023better}  &
\textbf{SynthCore} (this paper)\\ \hline
\textbf{Core Logic} & \textbf{Consensus}: Truth is the most frequent
answer. & \textbf{Evolution}: Optimal solutions are rare outliers
(mutations). \\ \hline
\textbf{Driver} & Decoding Sampling (Stochastic paths). & Input
Variation (Diverse seeds/contexts). \\ \hline
\textbf{Goal} & Minimize variance (Reliability). & Maximize variance
(Exploration). \\ \hline
\textbf{Selection} & Majority Vote (Mean/Mode). & Sorting/Ranking
(Best of). \\ \hline
\textbf{Analogy} & A jury voting to agree on a verdict. & Biological
mutations driving evolution. \\ \hline
\end{tabular}
\caption{Comparison of prompting strategies.}\label{consistent}
\end{table}


Our proposed \emph{SynthCore} method may appear superficially similar to self-consistency prompting, but as shown in Table~\ref{consistent} there are important distinctions. 
While both methods utilize multiple inference paths, they are
philosophically and mechanically distinct. \textbf{Self-Consistency}
acts as a consensus engine, filtering out noise to find the most
probable ``average'' truth \ ~\cite{ahmed2023better}. In contrast, \textbf{SynthCore}
acts as an evolutionary search engine, actively generating noise
(mutations) to discover superior ``outlier'' solutions.

Also, the two methods take different approaches to variation:
\begin{itemize}
    \item \textbf{Self-Consistency (Consensus):} Views variation as
    uncertainty to be minimized. It assumes correct reasoning paths
converge~\cite{ahmed2023better} . It relies on a voting mechanism to discard
    outliers and select the majority answer.
    \item \textbf{SynthCore (Evolution):} Views variation as a feature
    to be exploited. Drawing on seed generation theory \cite{xia2024fuzz4all}, it
    uses varied few-shot examples to force diverse outputs. These are
    treated as ``strong mutations'' in a search process, designed to
    escape local optima and find better warm-starts for active
    learners.
\end{itemize}
Also there are differences in how the  mechanism,
specifically decoding vs. input context.
This is to say that the methods intervene at different stages of the generation pipeline.
\begin{itemize}
    \item \textbf{Self-Consistency:} Keeps the input constant. It
    generates diversity solely by sampling multiple reasoning paths
    during the decoding stage \cite{wang2023selfConsistency}.
    \item \textbf{SynthCore:}  Varies the input context. It executes
    multiple independent sessions, each with unique randomized seeds
    or shuffled examples ~\cite{ahmed2023better}. This prevents the model from
    fixating on a single context window's logic.
\end{itemize}
Finally, the final answer is generated in a different way:  
\begin{itemize}
    \item \textbf{Self-Consistency:} Aggregates via \textbf{Voting}.
    The goal is to find the mode of the distribution (the most
    reliable answer)~\cite{ahmed2023better}.
    \item \textbf{SynthCore:} Aggregates via \textbf{Synthesis}. The
    goal is to collect all valid samples and sort them by an external
    objective function (e.g., Chebyshev distance) to find the single
    best candidate.
\end{itemize}
While self-consistency focuses on improving the \emph{consistency} of model outputs, our ensemble method emphasizes \emph{diversity}. The goal is to generate a diverse set of high-quality candidate outputs, which can be interpreted as producing strong ``mutations'' in a search process to move toward near-optimal solutions.

\BLACK
  
% \section{Literature Review}
% \subsection{Annotations in Software Engineering}\label{annotate}
% Manual annotations play a critical role in enabling and evaluating a wide range of data-driven Software Engineering (SE) tasks. As Easterbrook et al. emphasize, human annotators take a central to SE research, directly influencing the quality and impact of empirical findings \cite{easterbrook2008selecting}. 
% Over the years, manually annotated datasets have served as foundational resources for advancing research across various SE domains, including software defect prediction, vulnerability detection, sentiment analysis, code summarization, and the validation of static analysis warnings \here{R3AL}\BLUE (see, for example, the research related to Mining Software Repositories (MSR) and the PROMISE
% repository~\cite{menzies2025retrospective}).
% \BLACK
% A key advantage of manual annotation is the reliability it provides through redundancy: multiple annotators typically assess the same samples to ensure consistency and mitigate individual bias \cite{morse2002verification}. For instance, Patra et al. \cite{patra2022nalin} engaged 11 annotators to evaluate 40 Jupyter Notebook code samples for identifying name-value inconsistencies. Similarly, Kamp et al. \cite{kamp2019sesame} constructed a dataset measuring semantic similarity between Java methods mined from 11 repositories; each method type was annotated by 3 randomly selected annotators from a pool of 11. Kang et al. \cite{10.1145/3510003.3510214} used 2 annotators to label 1,306 code change samples into three classes, identifying whether the change addressed a corresponding static analysis warning. Hussain et al. \cite{hussain2008using} explored the use of manually annotated datasets to train models for identifying non-functional requirements (NFRs) from functional requirements within system requirements documentation. Their approach demonstrated high effectiveness, achieving an impressive 98.5\% accuracy in a 10-fold cross-validation test.

% On the other hand, the cost of employing multiple human annotators to ensure consistency and reliability can be substantial. In domains outside Software Engineering (SE), various automated annotation tools—particularly for tasks like sentiment analysis—have proven to be both effective and efficient, with many off-the-shelf solutions readily available. However, as Islam et al. point out, these generic tools often fail to perform well in domain-specific contexts like SE due to their inability to accurately interpret specialized terminology and semantic nuances \cite{7962370}. To address this, they propose SENTI-STRENGTH, a sentiment analysis model specifically tailored for the SE domain. It is trained on a corpus of 5,992 issue commit messages, each manually annotated by multiple reviewers. Their domain-specific model achieved notable performance, reporting an accuracy of 78\% and a recall of 85\%.


% The quality of these annotations significantly impacts model performance, particularly in domain-intensive SE tasks where subtle distinctions (e.g., between a feature request and a bug report) matter. However, despite their importance, manual annotations come with inherent challenges: 

% % We say these challenges fall into two areas:
% % \begin{itemize}
% % \item
% % {\em Domain challenges:} annotations requires expertise and expertise is very domain-specific.
% % Experts may be able to deliver useful
% % annotations in one area, but not in many others.
% % \item {\em Scalability challenges:}
% % As discussed in 
% % \S\ref{ops}
% % and \S\ref{ssec:human-factors}, manual annotations
% % suffer from problems associated with   human cognitive limits. 
% % \end{itemize}
% % \subsection{Challenges in using Manual  Annotations in SE} 
% % % This section highlights the practical challenges associated with using manual annotations in model-driven software engineering. It lays the groundwork for the key limitations and insights that are further explored in the subsequent sections of the paper.

% % %\subsubsection{Data Reliability Challenges in Empirical SE} \label{ssec:reliability}
% % Decades of software analytics research \cite{mockus2000identifying,hindle2008large} demonstrate that data quality concerns persist even in seemingly abundant datasets.
% % Data quality errors in annotations arising from incorrect labeling often lead to under performing models.
% %These errors arise in many ways:

% \begin{itemize}
% \item \textbf{Heuristic Labeling Pitfalls}: Common defect prediction practices label commits as bug-related through lexical patterns (e.g., "bug", "fix", "error") \cite{catolino2017just,kamei2012large,kim2008classifying}. However, Vasilescu et al. \cite{vasilescu2015quality,vasilescu2018personnel} demonstrate this approach yields inconsistent ground truth, particularly when researchers iteratively tune regular expressions without rigorous validation protocols.

% \item \textbf{Validation Discrepancies}: Human annotators
% often make many mistakes.
% For example,
% Yu et al.'s technical debt analysis \cite{9226105} revealed that 90\% of purported false positives actually represented  manual labeling errors. Similar patterns of errors in manual annotations are found in security defect identification \cite{9371393} and static analysis false positive classification \cite{10.1145/3510003.3510214}.
% \end{itemize}
% %\subsubsection{Human Annotation Throughput Limitations} \label{ssec:human-factors}
% Another problem with manual annotation is that it can be   remarkably slow:
% \begin{itemize}
% \item In Valerdi's \cite{valerdi2010heuristics} effort estimation study, he asked humans to label 60 examples according to 20 attributes. In order
% to achieve consensus on those labels, this tools three sets of three hour meetings hours (13.3 examples/hour)
% \item In Lustosa's \cite{lustosa2024learning} study,
% he asked humans to rank the software project management recommendations generated  from an interactive search-based SE tool. These humans performed this task in small chunks of time, mixed in with all their other work commitments. 
% For these humans, the   maximum sustainable labeling rate was 15-20 judgments/hour. Unlike
% the Valerdi study, this study did not require consensus among the human annotators. However, 
% in this case, human labeling was so slow
% since they were dealing with what
% Rittel calls a {\em wicked problem}~\cite{rittel1973dilemmas}
% \footnote{
% We say software management decision making is a {\em wicket problem} since it satisfies   the criteria set by Rittel\cite{rittel1973dilemmas}; i.e.
% (a)~their solutions are not true or false, only good or bad;
% (b)~thee problem cannot be studied through trial and error (i.e. solutions are irreversible so, as Rittel put it, “every trial counts.”);
% (c)~planners are liable for the consequences of the solutions they generate; the effects can matter a great deal to the people who are touched by those actions.}
% \item Easterbook and Kingston et al.~\cite{EASTERBYSMITH19803,kington2009defining}
% describe experiments with ``repertory grids'' 
% where humans had to annotate explore sets of three solutions to  proposing some  dimension along which one solution was most different to the other two. 
% In multiple studies, these researchers reports that takes humans one to two hours to annotate 10 examples with 10 attributes. 
% \end{itemize}
% As to operational constraints,
% field observations reveal additional throughput limitations:

% \begin{itemize}
% \item Our second author has  worked with industrial SMEs for many years~\cite{menzies1992expert}.
% He notes that experts are experts precisely since organizations rely on their expertise. Hence, they are often called away to important tasks.
% In practice, he found that  expert availability rarely exceeds 3 hours/week in industrial settings 
% \item It is unwise to demand that experts offer more time or faster annotations.
% Session duration $t$ inversely correlates with label quality~\cite{EASTERBYSMITH19803}.
% \end{itemize}
% As a final remark on human annotations, when we comment that "human annotations are slow",
% we are often asked about medical pathology labs that
% offer reports on thousands (or more)
% samples per day. That industry makes heavy use of automatic annotation agents, perhaps with humans reviewing a small random sample each day~\cite{krasowski2014autoverification}.
% In those reviews, human doctor annotators are looking a set of common errors within well-understood domains with widely-accepted definitions of   ``correct''
% and ``incorrect''~\cite{novis2006laboratory}. Annotation in software engineering, on the other hand, is far more subjective
% since (e.g.) different coders often have their own
% particular definitions of what is ``good'' programming style.


% \subsection{Can LLMs Replace Manual Annotators in SE?}
% Since manual annotation can be faulty or too slow,
% SE researchers have increasingly turned to automated annotation methods. \cite{dollmann2016and,bird2010linkster,nazar2023codelabeller,tan2011acomment} explores hybrid and fully automated approaches for generating quality annotations with reduced human effort. These methods aim to balance annotation cost with accuracy, enabling larger and more diverse datasets while minimizing the cognitive load on human experts.





 
% Large language models
% (LLMs) are a promising solution to address some of the limitations associated with traditional automated annotation tools. Unlike rule-based or narrow supervised models, LLMs are pre-trained on vast and diverse corpora, allowing them to better understand nuanced language and adapt to domain-specific contexts. Note that this approach can produce annotations without the need to extensive and expensive training on local data.



% For annotation purposes,
% LLMs are increasingly being used across various software engineering (SE) tasks, including code generation \cite{svyatkovskiy2020intellicode}, software testing \cite{deng2023large}, and more. With their advanced generative and reasoning capabilities, the range of potential use cases continues to expand rapidly. 
%  Ahmed et al. \cite{ahmed2024can} study (mentioned in
% our introduction) investigated whether or not LLMs can effectively replace manual annotation efforts, which are commonly used in empirical SE research. 
% % Such annotation tasks typically require significant human effort, domain expertise, and subjective judgment—making them expensive and difficult to scale.

% The Ahmed et al.  study explored two primary research questions:
% \begin{itemize}
%     \item Can LLMs, when guided by well-designed prompts, generate high-quality annotations?
%     \item Can multiple LLMs collectively simulate inter-rater agreement behaviors similar to those of human annotators?
% \end{itemize}
% To evaluate this, Ahmed et al.  assessed LLM performance on 10 annotation tasks across 5 publicly available software engineering datasets. These tasks encompass a wide range of real-world SE problems, including:

% \begin{itemize}
%     \item Automatic code summarization
%     \item Detection of name-value inconsistencies
%     \item Identification of causal relationships in non-functional requirements
%     \item Measuring semantic similarity between Java functions
%     \item Labeling static analysis warnings
% \end{itemize}
% Ahmed et al.  reported that LLMs perform surprisingly well in many of these annotation tasks. In \textbf{6 out of 10 tasks}, LLMs achieved \textbf{moderate to substantial agreement} with human annotators. Tasks that were well-structured and less context-dependent—such as identifying non-functional requirements or classifying code summaries—exhibited particularly strong agreement levels. Moreover, LLMs demonstrated consistent and stable behavior across repeated annotations, reinforcing their reliability for such tasks.

% However, the performance of LLMs declined significantly in more context-intensive tasks, such as identifying static analysis warnings and causal relationships. In these cases, LLMs showed low model-model agreement and fell short of human-level performance. Furthermore, the Ahmed et al.  study found that the hypothesized correlation between human-model and model-model agreement did not hold in the case of automated code summarization, illustrating a key limitation of relying solely on LLMs for annotation. These findings suggest that while LLMs are promising for certain annotation scenarios, they are not yet capable of fully replacing human annotators;especially for high dimensional or nuanced tasks.


% \subsection{Results from Related Work}\label{problem}

% In recent experiments on multi-objective optimization
% problems in SE, 
% Senthilkumar et al.\cite{senthilkumar2024can} arrived
% at analogous conclusions to Ahmed et al. 
% \begin{itemize}
% \item LLMs were very useful for 
% certain annotation scenarios (exploring low dimensional problems);
% \item LLMs did not work for all scenarios
% (higher dimensional problems).
% \end{itemize}
% The particular scenario explored by Senthilkumar et al.
% was using LLM annotations  to generate \textit{warm-start} samples for \textit{active learning} in \textit{multi-objective} SE tasks:

% \begin{itemize}

%   \item \textbf{Multi-objective reasoning:}  
%   Many SE problems involve trade-offs—e.g., maximizing accuracy while minimizing runtime or cost. Multi-objective reasoning addresses this by evaluating solutions across several often-conflicting goals. Rather than searching for a single best solution, the goal is  solutions where no objective can be improved without worsening another. This framework is central to many SE domains, such as architecture optimization~\cite{sayyad2013scalable}, configuration tuning~\cite{nair18}, and automated planning~\cite{harman2007search}. 

  
%   \item \textbf{Active learning:}  
%   Active learning iteratively builds a model from labeled data and selects the most informative unlabeled examples to query next. In the binary case, this might mean selecting data points near the current decision boundary—those that most confuse the model~\cite{settles2012active}. This strategy reduces the need for exhaustive labeling and has been explored in SE tasks such as defect prediction~\cite{tu2021frugal}.


%   \item \textbf{Warm-start samples:}  
%   Optimization algorithms often perform best when given a reasonable set of starting points. These initial inputs, called \textit{warm starts}, provide the learner with diverse and informative examples from which refinement can begin. 
% \end{itemize} 
% Active learners select their own training examples, allowing them to avoid (a) repetitive data, (b) irrelevant instances, and (c) highly unlikely outliers that are best left unexplored~\cite{settles2012active}. As a result, they often achieve strong performance using only a fraction of the labeled data required by traditional learners~\cite{lustosa2024learning,tu2021frugal}.

% Active learners help address the annotation issues discussed in \S\ref{annotate}. With fewer examples to label, analysts can focus more on label quality and complete their analysis faster by avoiding slow annotation bottlenecks.

% The Achilles' heel\footnote{An “Achilles' heel” is   a sole vulnerability that undermines an otherwise strong system.} of active learning is that it must select examples by reflecting on models built so far. To begin, an initial set of “warm start” examples is required. These can be chosen via (a)~random selection (more accurately, a “cold start”); (b)~diversity sampling\footnote{E.g., clustering the data and using the centroids.}; or (c)~expert input.
% This paper explores the idea that when experts are unavailable or costly, large language models (LLMs) can 
% be a useful surrogate for expert input.



% Senthilkumar et al.~\cite{senthilkumar2024can} showed that while LLM-generated warm-starts can help initiate the active learning process, they sometimes harm overall multi-objective optimization—especially in high-dimensional settings. Details of that study appear later, but we summarize the key findings here.

% % \begin{table}
% % \caption{Introducing the MOOT Repository. Available on line at} \url{http://github.com/timm/moot/optimize}.

% % % For more details on this data,
% % % see Table~\ref{dataset}.}\label{mooteg}

% % \small
% % \begin{tabular}{|p{\linewidth}|}\hline
% % MOOT (Multi-Objective Optimization Testing) is a collection of software engineering datasets drawn from tasks such as process tuning, database configuration, and hyperparameter optimization for, e.g.  defect prediction models.

% % All MOOT datasets follow form:

% % \begin{center}
% % \begin{minipage}{3in}
% % {\scriptsize
% % \begin{alltt}
% %   x = independent values          | y = dependent values
% %   --------------------------------|----------------------
% %   Spout_wait, Spliters, Counters, | Throughput+, Latency-
% %      10,         6,        17,    |    23075,    158.68
% %       8,         6,        17,    |    22887,    172.74
% %       9,         6,        17,    |    22799,    156.83
% %       9,         3,        17,    |    22430,    160.14
% %     ...,       ...,       ...,           ...,    ...
% %   10000,         1,        10,    |   460.81,    8761.6
% %   10000,         1,        18,    |   402.53,    8797.5
% %   10000,         1,        12,    |   365.07,    9098.9
% %   10000,         1,         1,    |   310.06,    9421
% % \end{alltt}
% % }
% % \end{minipage}
% % \end{center}


% % These tabular data sets are  are divided into independent ($x$) inputs and dependent ($y$) goals.
% % The first row names each column.
% % Numeric column names start with an uppercase letter.
% % Goal names ending in + or - are to be maximized or minimized, respectively.
% % For example, in the above table,  a system configures {\em Spout\_wait, Spliters, Counters} to maximize 
% % {\em Throughput} and minimize 
% % {\em Latency}. 

% % For illustration purposes, rows are sorted from best to worst based on those goals. But note that in our  experiments, rows are randomized and goal values (y) are initially hidden.\\\hline
% % \end{tabular}
% % \end{table}



% % The study explored dozens of software engineering (SE) multi-objective optimization tasks from the MOOT repository
% % (summarized in Table~\ref{mooteg}, with more details in 
% % Table~\ref{dataset})\footnote{MOOT = {\em Multi-Objective Optimization Tasks}, a collection of recent SE benchmark problems~\cite{Moot:2025}. \url{http://github.com/timm/moot}.}.  
% % In MOOT, data sets have:  
% %   \begin{itemize}
% %     \item 1 to 5 numeric optimization $y$ goals;
% %     \item 3 to 38 independent $x$ variables;
% %     \item 90 to 90,000 data rows.
% %   \end{itemize}
% % Following a recommendation from Di Fiore et al.~\cite{difiore2024}, the MOOT datasets are categorized based on their input ($x$) dimensionality:
% %   \begin{itemize}
% %     \item \textit{Low:} 12 datasets with $|x| < 6$ independent variables;
% %     \item \textit{Medium:} 14 datasets with $6 \le |x| \le 11$;
% %     \item \textit{High:} 19 datasets with $|x| > 11$
% %     \end{itemize}
% %     (Aside: We acknowledge that “high-dimensional” is defined differently in other domains.
% %     For example,   text mining may be considered low-dimensional relative to image processing. In defense of our
% %     categorization, we note that different studies
% %    report that active learning algorithms exhibit
% %     very different performance across data sets divided in this way~\cite{difiore2024,senthilkumar2024can}.)

% %   \begin{table}
% % \caption{Data used in this study.
% % As per  Di Fiore et al.~\cite{difiore2024},
% % data is labeled low, medium, or high dimensional
% % based on the number of $x$ independent goals. 
% %  $|x| < 6$ means "low" ;
% %  $6 \le |x| \le 11$ means "medium" and  $|x| > 11$  means "high".}\label{dataset}

 
% % \begin{center}
% % {\footnotesize
% % \begin{tabular}{p{1.7cm}|lrrl}  
% % \textbf{Groups} & \textbf{File Name} & $|$rows$|$ & \textbf{$|x|$ / $|y|$} & \textbf{Dimsionality} \\ \hline
% % \multirow{32}{*}{Configuration} 
% %     & SS-A    & 864    & 3/2  & low      \\
% %     & SS-B    & 972    & 3/2  & low      \\
% %     & SS-C    & 1080   & 3/2  & low      \\
% %     & SS-D    & 2736   & 3/2  & low      \\
% %     & SS-E    & 3008   & 3/2  & low      \\
% %     & SS-F    & 3839   & 3/2  & low      \\
% %     & SS-G    & 5184   & 3/2  & low      \\
% %     & SS-H    & 4608   & 4/2  & low      \\
% %     & SS-I    & 6840   & 5/2  & low      \\
% %     & SS-J    & 65536  & 6/2  & medium   \\
% %     & SS-K    & 86058  & 6/2  & medium   \\
% %     & SS-L    & 1023   & 11/2 & low      \\
% %     & SS-M    & 864    & 17/3 & high     \\
% %     & SS-N & 86058 & 18/2 & high \\
% %     & SS-O    & 972    & 11/2 & high     \\
% %     & SS-P    & 1023   & 11/2 & high     \\
% %     & SS-Q    & 2736   & 13/3 & high     \\
% %     & SS-R    & 3008   & 14/2 & high     \\
% %     & SS-S    & 3840   & 6/2  & medium   \\
% %     & SS-T    & 5184   & 12/2 & high     \\
% %     & SS-U    & 4608   & 21/2 & high     \\
% %     & SS-V    & 6840   & 16/2 & high     \\
% %     & SS-W    & 65536  & 16/2 & high     \\
% %     & SS-X    & 86058  & 11/3 & high     \\
% %     & Apache AllMeasurements & 192 & 9/1 & medium \\
% %     & SQL AllMeasurements    & 4653 & 38/1 & high \\
% %     & X264 AllMeasurements   & 1152 & 16/1 & high \\
% %     & rs-6d-c3 obj1          & 3840 & 6/1  & medium \\
% %     & rs-6d-c3 obj2          & 3840 & 6/1  & medium \\
% %     & sol-6d-c2-ob j1        & 2866 & 6/1  & medium \\
% %     & wc-6d-c1-ob j1         & 2880 & 6/1  & medium \\
% %     & wc+sol-3d-c4-ob j1     & 196  & 3/1  & low \\
% %     & wc+rs-3d-c4-obj1       & 196 & 3/1 & low\\
% %     & wc+wc-3d-c4-ob j1      & 196  & 3/1  & low \\
% %     \hline
% % \multirow{10}{*}{  Process  } 
% %     & pom3a          & 500    & 9/3 & medium \\
% %     & pom3b          & 500    & 9/3 & medium \\
% %     & pom3c          & 500    & 9/3 & medium \\
% %     & pom3d          & 500    & 9/3 & medium \\
% %     & xomo\_flight   & 10000  & 23/4 & high \\
% %     & xomo\_ground   & 10000  & 23/4 & high \\
% %     & xomo\_osp      & 10000  & 23/4 & high \\
% %     & xomo\_osp2     & 10000  & 23/4 & high \\
% %     & coc1000        & 1000   & 17/5 & high \\
% %     & nasa93dem      & 93     & 22/4 & high \\
% %     \hline
% % Project~Health 
% %     & healthCloseIsses12mths0001-hard & 10000 & 5/1 & low \\
% %     & healthCloseIsses12mths0011-easy.csv & 10000 & 5/1 & low \\
% %     \hline
% % \multirow{3}{*}{Miscellaneous} 
% %     & auto93         & 398    & 5/3 & low \\
% %     & Wine\_quality  & 1599   & 10/2 & medium \\
% %     & HSMGP num      & 3456   & 14/1 & high  
% % \end{tabular}}
% % \end{center}
% % \end{table}

% We define 
% {\em labeling} as the act of revealing a row’s y values. Labeling is often costly and/or error-prone
% (for all the reasons
% listed in \S\ref{annotate}). For
% example of Table~\ref{mooteg},
% it involves executing large test suites for each configuration. In other cases, it may require the attention
% of SMEs (who may be in short supply).
% Our work 
% aims to find methods that:

% \centerline{{\em Identify high-performing rows using as few labels as possible.}}

% \noindent Senthilkumar et al. documented baseline performances
% of their active learners using randomized cold-starts. These results
% were   compared to warm-starts found via  LLMs
% (via a few-shot prompting strategy,   described later in this paper).
% \noindent The results (which we repeat below in \S\ref{results})  were nuanced:
% \begin{itemize}
% \item
% On low-dimensional tasks, LLM-based warm-starts performed very well— often matching or surpassing traditional optimization methods. This initial result made us very
% hopeful that LLMs could become a viable alternative to
% traditional optimization methods.
% \item
% For medium-dimensional problems, LLMs were comparable to Gaussian Process and Tree-structured Parzen Estimator methods.
% \item
% But on high-dimensional problems, LLM-selected warm-starts underperformed—often falling below even random selection.
% \end{itemize}
% These results highlight a key limitation of LLMs in SE.
% While they are capable of generating plausible initial samples, they struggle with the intricacies of high-dimensional optimization problems.

% This raises a sobering question: can LLMs be trusted for anything beyond trivial optimization  problems? While they offer appealing performance on small-scale tasks, their failure to generalize to complex, high-dimensional domains suggests that—at least for now—LLMs may be little more than toys when it comes to serious scientific or engineering optimization.


  


% \subsection{Insight into why LLMs fail}
% To check if LLMs can be trusted beyond trivial optimization, first we must reflect
% on  the cause of their failure. 



% Large Language Models (LLMs) show considerable potential for generating effective initial solutions, or "warm starts," for active learning processes within software engineering (SE) optimization tasks \cite{chen2021evaluating}. However, the success of these models is heavily contingent upon the diversity and representativeness of their training data. When training data lacks sufficient variety—due to a scarcity of labeled examples, specific domain constraints, or inherent biases within the source datasets—LLMs may generate initial hypotheses that do not adequately explore the breadth of the optimization landscape \cite{bender2021dangers}. This limitation is particularly acute in multi-objective SE problems, where identifying optimal trade-offs among conflicting objectives often necessitates a diverse set of initial samples that can effectively map the Pareto front \cite{deb2002fast}.

% % In practical SE scenarios, curating diverse datasets is frequently challenging. The process of labeling SE data is often resource-intensive, demanding significant time, financial investment, and the input of subject matter experts whose availability is typically limited \cite{lin2023software}. Consequently, models trained on sparse or skewed datasets risk premature convergence to suboptimal solutions or may fail to identify more promising regions within the solution space \cite{he2009learningImbalanced}. This poses a critical question: how can these limitations be addressed when data diversity is inherently constrained?

% A promising strategy involves the deliberate cultivation of diversity during the inference phase, a concept we refer to as "diversity of inference." Instead of relying exclusively on diverse training datasets, this approach leverages techniques such as prompt ensembles to encourage LLMs and other inference engines to produce a broader spectrum of candidate solutions 

% % \cite{wang2023selfConsistency}. By systematically varying prompts or employing multiple inference paths, these methods can compel the model to explore less conventional areas of the solution space, thereby offering a means to compensate for deficiencies in training data diversity and enhance the robustness of SE optimization \cite{li2024surveyLLMCode}.

% % \BLUE
% % \here{R2a-2}
% % \subsection{Prompting Strategies}

% % A wide range of prompting strategies have been explored in recent literature~\cite{ahmed2022few,le2023log,nashid2023retrieval,li2025structured,yang2024chain,ahmed2023better,shinn2023reflexion}. These strategies vary depending on the nature of the problem space, but several have emerged as particularly influential, including \emph{few-shot prompting}, \emph{chain-of-thought prompting}, \emph{self-consistency}, and \emph{self-reflection} approaches.
% % Figure ~\ref{prompt} provides an overview on each of the above methods.


% % Few-shot prompting~\cite{ahmed2022few,le2023log,nashid2023retrieval} is the standard practice of providing a small number of labeled examples (typically $n = 1$ to $4$--$5$) within the prompt. This helps large language models generalize to previously unseen tasks. Many advanced prompting techniques can be viewed as extensions or modifications of the few-shot paradigm.


% % Chain-of-thought (CoT) prompting~\cite{li2025structured,yang2024chain} extends few-shot prompting by including step-by-step explanations or reasoning traces along with each example. This encourages the model to follow explicit reasoning structures when solving unseen tasks, often improving interpretability and output quality. However, CoT has a notable drawback: if the provided reasoning path is incorrect or biased, the model may confidently follow and amplify flawed logic.


% % Self-consistency prompting~\cite{ahmed2023better} addresses the limitations of standard CoT. Instead of generating a single explanation--answer pair, the model generates multiple reasoning paths during decoding. These paths are then aggregated, typically via a voting mechanism, to select the most reliable final answer. This reduces sensitivity to any single, potentially incorrect chain of reasoning.


% % Self-reflection prompting~\cite{shinn2023reflexion} represents a distinct strategy in which the model iteratively refines its responses. In this setting, the model evaluates its previous outputs across a sequence of interactions (question $\rightarrow$ answer $\rightarrow$ critique $\rightarrow$ revised answer), enabling incremental improvement across iterations.

% % \begin{table}[!t]
% % \centering
% % \BLUE
% % \footnotesize
% % \begin{tabular}{|p{2cm}|p{4cm}|p{4cm}|}
% % \hline
% % \textbf{Feature} & \textbf{Self-Consistency} ~\cite{ahmed2023better}  &
% % \textbf{SynthCore} (this paper)\\ \hline
% % \textbf{Core Logic} & \textbf{Consensus}: Truth is the most frequent
% % answer. & \textbf{Evolution}: Optimal solutions are rare outliers
% % (mutations). \\ \hline
% % \textbf{Driver} & Decoding Sampling (Stochastic paths). & Input
% % Variation (Diverse seeds/contexts). \\ \hline
% % \textbf{Goal} & Minimize variance (Reliability). & Maximize variance
% % (Exploration). \\ \hline
% % \textbf{Selection} & Majority Vote (Mean/Mode). & Sorting/Ranking
% % (Best of). \\ \hline
% % \textbf{Analogy} & A jury voting to agree on a verdict. & Biological
% % mutations driving evolution. \\ \hline
% % \end{tabular}
% % \caption{Comparison of prompting strategies.}
% % \end{table}


% % Our proposed \emph{SynthCore} method may appear superficially similar to self-consistency prompting, but there are important distinctions. 
% % While both methods utilize multiple inference paths, they are
% % philosophically and mechanically distinct. \textbf{Self-Consistency}
% % acts as a consensus engine, filtering out noise to find the most
% % probable ``average'' truth \ ~\cite{ahmed2023better}. In contrast, \textbf{SynthCore}
% % acts as an evolutionary search engine, actively generating noise
% % (mutations) to discover superior ``outlier'' solutions.

% % Also, the two methods take different approaches to variation:
% % \begin{itemize}
% %     \item \textbf{Self-Consistency (Consensus):} Views variation as
% %     uncertainty to be minimized. It assumes correct reasoning paths
% % converge~\cite{ahmed2023better} . It relies on a voting mechanism to discard
% %     outliers and select the majority answer.
% %     \item \textbf{SynthCore (Evolution):} Views variation as a feature
% %     to be exploited. Drawing on seed generation theory \cite{xia2024fuzz4all}, it
% %     uses varied few-shot examples to force diverse outputs. These are
% %     treated as ``strong mutations'' in a search process, designed to
% %     escape local optima and find better warm-starts for active
% %     learners.
% % \end{itemize}

% % Also there are differences in how the  mechanism,
% % specifically decoding vs. input context.
% % This is to say that the methods intervene at different stages of the generation pipeline.

% % \begin{itemize}
% %     \item \textbf{Self-Consistency:} Keeps the input constant. It
% %     generates diversity solely by sampling multiple reasoning paths
% %     during the decoding stage \cite{369}.
% %     \item \textbf{SynthCore:}  Varies the input context. It executes
% %     multiple independent sessions, each with unique randomized seeds
% %     or shuffled examples ~\cite{ahmed2023better}. This prevents the model from
% %     fixating on a single context window's logic.
% % \end{itemize}

% % Finally, the final answer is generated in a different way:  
% % \begin{itemize}
% %     \item \textbf{Self-Consistency:} Aggregates via \textbf{Voting}.
% %     The goal is to find the mode of the distribution (the most
% %     reliable answer)~\cite{ahmed2023better}.
% %     \item \textbf{SynthCore:} Aggregates via \textbf{Synthesis}. The
% %     goal is to collect all valid samples and sort them by an external
% %     objective function (e.g., Chebyshev distance) to find the single
% %     best candidate.
% % \end{itemize}


% % While self-consistency focuses on improving the \emph{consistency} of model outputs, our ensemble method emphasizes \emph{diversity}. The goal is to generate a diverse set of high-quality candidate outputs, which can be interpreted as producing strong ``mutations'' in a search process to move toward near-optimal solutions.

% % \BLACK


% \subsection{Can ``Ensemble Learning'' Repair Failing LLMs?}\label{whyensemble}

% As shown above, large language models (LLMs) often struggle with consistency and reliability in specialized domains, which makes robust enhancement techniques essential. 
% {\em  Ensemble learning} is a well-established approach that boosts robustness, predictive accuracy, and generalization, especially under data-scarce, few-shot learning conditions \cite{dietterich2000ensemble,sagi2018ensemble}. By training multiple diverse models and combining their outputs, ensembles typically outperform single learners: they resist noise and outliers more effectively and generalize better on high-dimensional datasets where individual models often fail \cite{dietterich2000ensemble}.

% % The inherent limitations of Large Language Models (LLMs), particularly concerning their consistency and reliability in specialized applications, necessitate the adoption of robust enhancement strategies. Ensemble learning stands out as a widely recognized and effective methodology for improving model robustness, predictive accuracy, and generalization capabilities, especially in scenarios characterized by data scarcity, such as few-shot learning \cite{dietterich2000ensemble, sagi2018ensemble}. The foundational principle of ensemble learning involves training multiple diverse models and combining their outputs, which typically yields superior performance compared to individual learners. Ensembles are particularly noted for their resilience to noise and outliers in training data and for their enhanced generalization on high-dimensional datasets where single models may falter \cite{dietterich2000ensemble}.

% A canonical example of ensemble learning is the Random Forest algorithm, which comprises numerous decision trees \cite{breiman2001randomForests}. Each tree in the forest is trained on a bootstrapped subset of the original data, and at each node, splits are determined from a random subset of the available features (attributes). The final prediction of the Random Forest is derived by aggregating the predictions (e.g., through majority voting for classification or averaging for regression) from all constituent trees, contributing to its strong performance and robustness.

% In the context of few-shot learning (FSL), where models must learn from a very limited number of examples, conventional methods can exhibit high variance in performance. Ensemble techniques are increasingly proposed and utilized to mitigate this issue and enhance the stability of FSL models \cite{lin2022ensembleSurveyFSL}. Several recent works have demonstrated the efficacy of applying ensemble principles to LLMs in few-shot scenarios \cite{wang2023selfConsistency, lin2022ensembleSurveyFSL, lau2024dipper, pitis2023boostedPrompting}.

% For two reasons, we conjecture that ensembles can help the annotation
% of higher dimensional multi-objective
% problems:

% \begin{itemize}
%     \item \textit{Diversity in Ensembles for Generalization:} Achieving diversity among ensemble members is critical for performance. Lau et al. \cite{lau2024dipper} advocate for methods that promote this diversity, such as employing varied prompts or diverse example sets during the prompting process for LLMs, to foster improved generalization.

%     \item \textit{Aggregation of Multiple Reasoning Paths:} Wang et al. \cite{wang2023selfConsistency} introduced self-consistency, a method that involves sampling multiple reasoning paths from an LLM during the decoding stage and then selecting the most consistent output. This approach has been shown to significantly improve performance on complex reasoning tasks by exploring diverse solutions before aggregation.

%     % \item \textit{Iterative Refinement and Boosting-like Mechanisms:} Techniques akin to boosting, such as reflective prompting, incorporate feedback from prior input-output interactions to refine subsequent prompts and model responses. While conceptually distinct from traditional bagging-based ensembles, these iterative methods can be effectively integrated with ensemble strategies to further enhance the quality and reliability of LLM outputs \cite{pitis2023boostedPrompting}.
% \end{itemize}




% \section{Background}
% \subsection{Motivation} 
% In 2024, we explored the use of LLMs for multi-objective optimization by generating warm-start samples for a range of datasets. These datasets were drawn from various SE optimization problems, including cloud services, data mining, and software process configurations. They varied significantly in size and dimensionality, containing:

% \begin{itemize} \item Between 93 and 86,000 rows, \item 3 to 38 independent ($x$) variables, and \item 1 to 5 dependent ($y$) variables. \end{itemize}

% Our initial approach used LLMs to generate initial samples for active learning-based optimizers. While this strategy worked well for low and medium-dimensional datasets, it struggled with high-dimensional datasets. This discrepancy could be due to:

% \begin{itemize} \item LLMs' difficulty in synthesizing high-quality configurations for  high-dimensional spaces. \item The possibility of more effective ways to leverage LLMs for generating initial samples — a topic worth further exploration. \end{itemize}

% To address these challenges, we have shifted to an ensemble approach of LLM few-shot learners. This method has shown promise in mitigating the diminishing returns of single-shot methods, offering more robust solutions across a wider variety of SE optimization problems.


% \subsection{Why SE data is scarce?} \label{sec:data-scarcity}
% This section establishes the theoretical justification for few-example evaluation paradigms by systematically analyzing three fundamental constraints inherent to software engineering data ecosystems.

% \subsubsection{Data Reliability Challenges in Empirical SE} \label{ssec:reliability}
% Four decades of software analytics research \cite{mockus2000identifying,hindle2008large} demonstrate that data quality concerns persist even in seemingly abundant datasets. These issues manifest through multiple pathways:

% \begin{itemize}
% \item \textbf{Heuristic Labeling Pitfalls}: Common defect prediction practices label commits as bug-related through lexical patterns (e.g., "bug", "fix", "error") \cite{catolino2017just,kamei2012large,kim2008classifying}. However, Vasilescu et al. \cite{vasilescu2015quality,vasilescu2018personnel} demonstrate this approach yields inconsistent ground truth, particularly when researchers iteratively tune regular expressions without rigorous validation protocols.

% \item \textbf{Validation Discrepancies}: Yu et al.'s technical debt analysis \cite{9226105} revealed that 90\% of purported false positives actually represented labeling errors. Similar patterns emerge in security defect identification \cite{9371393} and static analysis false positive classification \cite{10.1145/3510003.3510214}.

% \item \textbf{Cleansing Overhead}: Faulty data necessitates labor-intensive curation processes. Our methodology reduces required training samples from hundreds to dozens, thereby decreasing cleansing effort by 83-92\% based on prior cleanup cost models \cite{menzies1999critical}.
% \end{itemize}

% \subsubsection{Asymmetric Data Acquisition Costs} \label{ssec:asymmetry}
% Software engineering exhibits unique economic characteristics in feature ($x$) versus label ($y$) collection, as formalized in Table~\ref{slow}. Let $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ represent a dataset where:

% \begin{itemize}
% \item $x \in \mathbb{R}^d$: Easily obtainable independent variables (code metrics, configuration parameters)
% \item $y \in \mathbb{R}^m$: Costly dependent variables requiring human/expert input
% \end{itemize}

% \begin{table}[!t]
% \centering
% \caption{Feature-Label Cost Disparity in SE Tasks}
% \label{slow}
% \rowcolors{2}{gray!15}{white}
% \begin{tabular}{p{2.6in}p{2.6in}}
% \hline
% \textbf{Low-Cost Features (x)} & \textbf{High-Cost Labels (y)} \\
% \hline
% Mining GitHub for code complexity metrics & Establishing commercial valuation of software artifacts \\
% \hline
% Generating configuration permutations & Human validation of design options through stakeholder workshops \\
% \hline
% Automated test case generation & Runtime verification and anomaly detection in CI pipelines \\
% \hline
% Extracting project dependency graphs & Quantifying team effort through organizational audits \\
% \hline
% \end{tabular}
% \end{table}

% This asymmetry creates a data acquisition bottleneck where $|\mathcal{X}| \gg |\mathcal{Y}|$. Our approach directly addresses this imbalance through sample-efficient learning frameworks.

% \subsubsection{Human Annotation Throughput Limitations} \label{ssec:human-factors}
% Expert-driven data collection exhibits strict upper bounds due to cognitive and logistical constraints:

% \subsubsubsection{Labeling Rate Analysis}
% Empirical studies establish consistent human annotation ceilings:

% \begin{itemize}
% \item Valerdi's \cite{valerdi2010heuristics} effort estimation study: 60 examples $\times$ 20 attributes required 9 person-hours (13.3 examples/hour)
% \item SNEAK iSBSE tool \cite{lustosa2024learning}: Maximum sustainable labeling rate = 15-20 judgments/hour
% \item Repertory grid techniques \cite{EASTERBYSMITH19803,kington2009defining}: 10$\times$10 grid completion $\approx$ 2 hours
% \end{itemize}

% \subsubsection{Operational Constraints}
% Field observations reveal additional throughput limitations:

% \begin{itemize}
% \item Expert availability rarely exceeds 3 hours/week in industrial settings \cite{menzies1992expert}
% \item Session duration inversely correlates with label quality ($r = -0.72$, $p < 0.01$) \cite{EASTERBYSMITH19803}
% \item Inter-annotator disagreement increases exponentially beyond 25 labels/session \cite{9371393}
% \end{itemize}

% These constraints make comprehensive dataset creation infeasible for emerging SE challenges like AI-enhanced code review \cite{9226105} or quantum software verification \cite{10.1145/3510003.3510214}, necessitating data-efficient evaluation strategies.

% % \subsubsection{Methodological Implications}
% % The tripartite challenge of reliability, asymmetry, and human factors establishes strict bounds on practical data availability:

% % \begin{equation}
% % \mathcal{A}_{\text{available}} = \min\left(\mathcal{R}_{\text{reliability}} \cdot \mathcal{D}_{\text{total}}, \frac{\mathcal{C}_{\text{human}}}{\tau_{\text{label}}}\right)
% % \end{equation}

% % Where $\mathcal{R}_{\text{reliability}}$ = reliable data fraction, $\mathcal{C}_{\text{human}}$ = available expert hours, and $\tau_{\text{label}}$ = time per label. Our evaluation framework operates effectively when $\mathcal{A}_{\text{available}} \leq 50$, as demonstrated in \S4.


% \subsection{Addressing Data Scarcity in Software Engineering}

% The pervasive challenge of data scarcity in software engineering has prompted extensive research and the development of various innovative approaches. This section examines five primary methodologies designed to mitigate data shortages: label-less learning, semi-supervised learning, few-shot learning with Large Language Models (LLMs), active learning, and Gaussian Process Models (GPM) with Tree of Parzen Estimators (TPE).

% \subsubsection{Unsupervised Learning} \label{ssec:label-less}
% Unsupervised learning techniques employ domain-specific heuristics to classify examples based solely on independent x variables. Notable examples include:

% \begin{itemize}
%     \item Nam et al.'s approach to defect prediction by identifying unusually large classes across multiple dimensions \cite{nam2015clami}.
%     \item Failure detection through core dump analysis or metamorphic testing predicates \cite{chen2018metamorphic}.
% \end{itemize}

% However, the efficacy of unsupervised learning is contingent upon the availability of relevant domain heuristics, which may not exist for all problem spaces.




% \subsubsection{Semi-Supervised Learning Techniques} \label{ssec:semi-supervised}

% Semi-supervised learning methodologies aim to propagate labels from a small subset of labeled examples to a larger unlabeled dataset. Yehida et al. \cite{yedida2023find} demonstrate this approach through recursive bi-clustering of N examples into leaf clusters of size $N^0.25$, assigning labels based on cluster centroids.

% While semi-supervised learning has shown success in processing large datasets (10,000+ records) with minimal labeling (1-2.5\%) \cite{10109333,majumder2024less}, it still requires hundreds to thousands of labels. This demand exceeds the practical constraints outlined in previous sections, necessitating more efficient approaches.

% .


% \subsubsection{Few Shot learning} Few shot learning is a meta learning approach where the model makes accurate predictions with very limited data. Unlike pre training or fine tuning a language model that requires billions and trillions of tokens worth of training data few shot learning leverages the generalization capabilities and pre-existing knowledge of the language model to make informed predictions \cite{brown2020language}.

% Several SE researchers have explored the usage of few shot learning across various tasks. However, only limited papers have used few shot learning for SE tasks and found success \cite{chaaben2023towards,ahmed2022few,colavito2023few}. \cite{nashid2023retrieval} employed few shot learning for automated assertion generation and program repair. \cite{tawosi2023search} used genetic algorithms to select few shot examples for story point estimation of the projects from the TAWOS dataset \cite{tawosi2022versatile}. \cite{akli2023flakycat} used a CodeBert Based few shot learner to classify flaky tests from a set of 451 flaky tests collected from java projects. Figure \ref{lables-fsl} shows the number of labels required for different few shot learners across SE tasks mentioned above. The chart highlights that for most of the tasks few shot learners require more than 100s of labeled samples to better traditional methods.   

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{LaTeX_DL_468198_240419/Number of Labels required for Few shot learners.png}
%     \caption{Caption}
%     \label{lables-fsl}
% \end{figure}

% \subsection{Challenges in LLMs for SE}

% \subsubsection{Theoretical Challenges}
% A recent survey from Gao et.al \cite{gao2024current} suggests great potential in (LLM4SE) however highlighting the setbacks LLMs face in uncommon tasks with limited or problematic data. A discussion with 24 experts identified 26 key challenges for LLMs across areas like software requirement \& design, coding assistance, testing, code review, maintenance, vulnerability management, and data handling. Majority of these tasks underline incompatible data issues for training LLMs thereby addressing a need for extensive research in usage of LLMs in these tasks. 

% \begin{table}[ht]
% \centering
% \caption{Key LLM Challenge Categories in Uncommon SE Tasks according to Gao et.al}
% \label{tab:challenges}
% \begin{tabular}{lll}  
% \hline
% \textbf{Domain} & \textbf{Top Challenges} \\  
% \hline  
% Requirement Engineering & Contextual ambiguity resolution  \\  
% Testing & False positive identification \\  
% Vulnerability Mgmt. & Execution data dependency  \\  
% Maintenance & Cross-file dependency tracking \\  
% \hline  
% \end{tabular}
% \end{table}


% \subsubsection{Empirical Performance Gaps}  
% Despite advancements, recent benchmarks highlight significant limitations in LLM capabilities:  

% \begin{itemize}  
% \item \textbf{Data Annotations}: LLMs fail on context-heavy tasks like triaging static analysis alerts \cite{ahmed2024can}, falling short of human annotators. 
% Ahmed et al. demonstrated that LLMs can serve as a viable replacements for human annotators in software engineering tasks \cite{ahmed2024can}. The tasks from the study included  automatic code summarization, name value inconsistencies in code, semantic similarity between functions, causality and static analysis warnings. Through a novel metric, model-model agreement the methods used significantly reduced the human annotation effort by 33\% for few of the above tasks. However despite its strengths, LLMs failed to perform in more subjective and context heavy tasks like static analysis warnings and causality. Additionaly, their premise of correlation between human-model agreement to model-model agreement was not true for annotating code summarization, highlighting clear limitations of LLMs.

% \item \textbf{Vulnerability Detection and Patching}: Despite the advancements in state-of-the-art models, their performance on Software Vulnerability Detection (SVD) and Software Vulnerability Patching (SVP) tasks remains no better than random guessing. Zibaeirad et al. \cite{zibaeirad2024vulnllmeval} highlighted the limitations of LLMs in identifying vulnerable and patched code blocks across various prompts. The study evaluated several leading models, including Llama3, Codellama, Mistral, and the Gemma family. The results indicate that even these sophisticated models struggle to effectively distinguish between vulnerable and patched code.


% \item \textbf{Test Case Generation:}
% Sidique et al. \cite{siddiq2023exploring} evaluated the effectiveness of LLMs in generating unit test cases for programs from two benchmarks: HumanEval and Evosuite SF110. Their findings reveal a significant disparity in performance — the generated test cases achieved 80\% code coverage for the more popular HumanEval dataset but only 2\% coverage for the less popular SF110 benchmark. This limited coverage for SF110 often led to issues like code smells and inadequate test scenarios, highlighting the challenges LLMs face in less common, underrepresented datasets.
% \end{itemize}
% \\



% These limitations map to a clear pattern: LLMs excel at localized syntactic tasks (e.g., code completion: 84\% accuracy \cite{llm4se2025}) but struggle with tasks not present in commons. This dichotomy suggests future research should focus on better methods to expand LLMs' capacity for reasoning beyond memorized knowledge, improve their ability to generalize in low-resource scenarios










% \subsection{Ensemble Learning}

% The challenges highlighted in the above section underscore the limitations of LLMs, necessitating strategies that enhance their reliability. One promising approach is ensemble learning, which has been widely adopted to improve model robustness and generalization, particularly in data-scarce scenarios like few-shot learning. Ensemble learning trains multiple models on different subsets of the data to improve performances over traditional learners. Ensemble learners are robust to noises and outliers in data making them more reliable, these models generalize better on higher dimensional data, where the performance of single models are impacted. 

% Random forests are classic examples of ensemble learners composed of multiple decision trees trained on randomly selected $\sqrt[]{A}$ samples of attribute A. The final prediction of random forests is provided using the aggregates of the predicted values of all individual learners.

% \cite{wang2022self,lin2022ensemble,lau2024dipper, pitis2023boosted} have employed ensemble learning in few shot learning scenarios. \cite{lin2022ensemble} suggests the use of ensemble learners for few shot learners as the conventional few shot methods are high in variance.  

% \subsubsection{Key Themes and Approaches in Ensembles of Few-Shot Learners:}

% \bi

% \item \em{Diversity in Ensembles}: Lau et al. \cite{lau2024dipper} emphasize that diversity is essential for strong ensemble performance. This can be achieved by using diverse examples on different prompts, thereby providing better generalization.

% \item \em{Multiple Reasoning Paths}: Wang et al. \cite{wang2022self} demonstrated that aggregating multiple reasoning paths in LLMs improves higher dimensional reasoning tasks. Unlike previous approaches that focused on prompt diversity, their method integrates ensembles at the decoding stage, leading to more effective LLM responses.

% \item \em{Reflective Prompting}: A core aspect of reflective prompting is incorporating feedback from previous input-output interactions into subsequent prompt iterations. While this approach aligns more closely with boosting than traditional ensembles, it has been effectively combined with ensemble methods to enhance response quality \cite{pitis2023boosted}.

% \ei

% Using the ideas from above methods as inspiration, we explore the usage of ensemble of one shot prompts for synthesizing optimal candidate points in active learning. 




\section{Methods}

Drawing inspiration from these established and emerging ensemble methodologies, the rest of this paper investigates the application of ensembles of few-shot prompts for the synthesis of optimal candidate solutions within active learning frameworks. 

This section describes the data, algorithms, performance measures used in this study.



\subsection{Data}\label{data}

 

\begin{table}
\caption{Introducing the MOOT Repository. Available on line at \url{http://tiny.cc/moot}.}\label{mooteg}
% For more details on this data,
% see Table~\ref{dataset}.}\label{mooteg}

\small
\begin{tabular}{|p{\linewidth}|}\hline
MOOT (Multi-Objective Optimization Testing) is a collection of software engineering datasets drawn from tasks such as process tuning, database configuration, and hyperparameter optimization for, e.g.  defect prediction models.

Table~\ref{mooteg} shows the general
form of MOOT data.  

\begin{center}
\begin{minipage}{3in}
{\scriptsize
\begin{alltt}
  x = independent values          | y = dependent values
  --------------------------------|----------------------
  Spout_wait, Spliters, Counters, | Throughput+, Latency-
     10,         6,        17,    |    23075,    158.68
      8,         6,        17,    |    22887,    172.74
      9,         6,        17,    |    22799,    156.83
      9,         3,        17,    |    22430,    160.14
    ...,       ...,       ...,           ...,    ...
  10000,         1,        10,    |   460.81,    8761.6
  10000,         1,        18,    |   402.53,    8797.5
  10000,         1,        12,    |   365.07,    9098.9
  10000,         1,         1,    |   310.06,    9421
\end{alltt}
}
\end{minipage}
\end{center}


These tabular data sets are  are divided into independent ($x$) inputs and dependent ($y$) goals.
The first row names each column.
Numeric column names start with an uppercase letter.
Goal names ending in + or - are to be maximized or minimized, respectively.
For example, in the above table,  a system configures {\em Spout\_wait, Spliters, Counters} to maximize 
{\em Throughput} and minimize 
{\em Latency}. 

For illustration purposes, rows are sorted from best to worst based on those goals. But note that in our  experiments, rows are randomized and goal values (y) are initially hidden.\\\hline
\end{tabular}
\end{table}

This study explored dozens of software engineering (SE) multi-objective optimization tasks from the MOOT repository~\cite{Moot:2025}
shown in Table~\ref{dataset}
(with more details in 
Table~\ref{mooteg})\footnote{MOOT = {\em Multi-Objective Optimization Tasks}, a collection of recent SE benchmark problems~\cite{Moot:2025}. \url{http://tiny.cc/moot}.}.  
In MOOT, data sets have:  
  \begin{itemize}
    \item 1 to 5 numeric optimization $y$ goals;
    \item 3 to 38 independent $x$ variables;
    \item 90 to 90,000 data rows.
  \end{itemize}
Following a recommendation from Di Fiore et al.~\cite{difiore2024}, the MOOT datasets are categorized based on their input ($x$) dimensionality:
  \begin{itemize}
    \item \textit{Low:} 12 datasets with $|x| < 6$ independent variables;
    \item \textit{Medium:} 14 datasets with $6 \le |x| \le 11$;
    \item \textit{High:} 19 datasets with $|x| > 11$
    \end{itemize}
    (Aside: We acknowledge that “high-dimensional” is defined differently in other domains.
    For example,   text mining may be considered low-dimensional relative to image processing. In defense of our
    categorization, we note that different studies
   report that active learning algorithms exhibit
    very different performance across data sets divided in this way~\cite{difiore2024,senthilkumar2024can}.)


This MOOT data  divides into four groups:
\begin{enumerate}
\item The {\em Config} directory consists of datasets derived from software engineering literature, specifically those labeled with "SS-*" \cite{nair2016accidental}. These datasets comprehensively capture configurations for various tasks, such as video encoding, with primary objectives including query time and run time. Additionally, Config includes datasets related to database configurations, such as Apache\_AllMeasurements.csv, SQL\_ALLMeasurements.csv, and \newline
X264\_AllMeasurements.csv.  

\item The {\em HPO} directory houses datasets from the hyperparameter optimization domain \cite{lustosa2024learning}. These datasets document the outcomes of random forest regression models trained to forecast metrics such as commits, closed issues, and closed pull requests over a 12-month period for open-source projects hosted on GitHub. The Y-values in these datasets represent the error and accuracy of the model under different hyperparameter settings.  

\item The {\em Process} directory contains datasets originating from software process modeling research \cite{green2009understanding, Me07, menzies2009avoid, port2008using}. The "pom*" datasets capture insights into agile development as studied by \cite{boehm2004balancing}. Specifically, the POM3 model represents requirements as a tree of dependencies that emerge dynamically, akin to elements surfacing from water. This model tracks completion rates, idle times, and development effort as teams navigate evolving tasks. Additionally, the "nasdem" dataset provides real-world data, while "osp2" follows the USC Cocomo model format, offering predictions for development effort, defect rates, and risks in waterfall-style software projects \cite{Me07}.  

\item The {\em Misc} directory includes non-software engineering datasets, such as auto93 and WineQuality, which serve as demonstration tools for presenting MOOT to a broader audience.  
\end{enumerate}
We provide   other meta data on the datasets externally.
\footnote{
\BLUE
\here{R4b}, \here{R4c} Details of the parameters for every dataset are provided here: \url{https://github.com/lohithsowmiyan/lazy-llm/blob/clusters/docs/datasets.pdf}
\BLACK
}





\subsection{Performance Measure}
Our active learners returned a row of options.
To measure the effectiveness
of that row, 
  we use the   {\em
Chebyshev Distance}. This is the  maximum distance between any   
$y$ value of a row to its ideal $y$ value in the dataset. 

  \begin{equation}\label{eq:ch}
d_\text{Chebyshev}(y, o) = \max_{i=1, \ldots, n} \left| y_i - l_i \right|
\end{equation}
For this calculation, we normalized the $y$ values for each goal to 0..1 min..max.
For the data set shown in Table~\ref{mooteg}, those ideal $y$ values are associated with maximal {\em Throughput} and minimal {\em Latency}; i.e. their ideal $y$ values are:
\[
\mathit{ideal} \; \{\mathit{Throughput}, \mathit{Latency}\} = \{1, 0\}
\]
We use Chebyshev since:
\begin{itemize}
\item It is also used by other prominent multi-objective algorithms~\cite{zhang2007moea};
\item It is a ``cruel critique'' that punishes   failure for any optimization goal;
\item We have run all our results with other evaluation measures (e.g. average distance of row goals to the ideal) and our main result
(that SynthCore works for higher dimensional data) still persists.
\end{itemize}


\subsection{ Active Learners}
  
Given   $n$ initially labeled examples, active learning proceeds as follows~\cite{brochu2010tutorial,lustosa2024learning}.:

\begin{enumerate}
\item Acquire the next most informative example based on the current model.
\item Label this example and add it to the training set.
\item Update the predictive model.
\item Repeat until the labeling budget ($B$) is exhausted.
\item Return the \textbf{best} example (where ``best'' is defined as per Equation~\ref{eq:ch}).
\end{enumerate}
Due to their prominence in the literature, we  focus on two   active learners:

\subsubsection{Gaussian Process Models (GPM)} \label{ssec:gpm}

GPMs generate estimates by passing the available data through a range of kernels. In this way, they can
generate a  mean ($\mu$) and standard deviation ($\sigma$) for each prediction\cite{williams1995gaussian,brochu2010tutorial}. The   Upper Confidence Bound (UCB) acquisition function cab use $\mu,\sigma$ to guide the selection of the next
example to label. UCB recommends labeling the example $x$ that maximizes:


\begin{equation}
UCB(x) = \argmax_x (\mu(x) + \kappa \sigma(x))
\end{equation}
(where $\kappa$ is a constant).
Early in the reasoning, when little is known, the variances are large and the $\kappa \sigma(x)$ term dominates. Later,
as more data reduces the variance, 
UCB converges to just $\mu(x)$.
In this way, UCB adapts from
{\em exploring} regions of large variance
to {\em exploiting} regions with best mean prediction.

\subsubsection{Tree-structured Parzen Estimator (TPE)} \label{ssec:tpe}

The Tree-structured Parzen Estimator (TPE) differs from Gaussian Process methods by separately modeling the conditional distributions $p(x|y)$ and the marginal distribution $p(y)$ \cite{bergstra2011algorithms}. TPE splits the data based on objective values into two distinct groups, defined by a threshold $y^*$. Specifically, the conditional probability is represented as:

\begin{equation}
p(x|y) = \begin{cases}
l(x), & \text{if } y < y^* \\
g(x), & \text{if } y \geq y^*
\end{cases}
\end{equation}
Here, $l(x)$ represents the distribution of high-performing
observations, whereas $g(x)$ corresponds to the lower-performing
observations. The threshold $y^*$ is determined using a quantile
$\gamma$, ensuring $p(y < y^*) = \gamma$. The primary goal of TPE
is to select configurations that maximize the probability under
$l(x)$ and minimize it under $g(x)$, optimizing:
\begin{equation}
\arg\min_x \frac{g(x)}{l(x)}
\end{equation}


% Independent variables in MOOT contains both numerical and symbolic data while the dependent variables are only numeric. Every row contains a header with names self descriptive of the feature, numeric columns are denoted by feature names starting with an upper case while symbolic columns by feature names starting with a lower case. '+' and '-' symbols at the end of dependent variable names indicate whether the objective of the feature is to either maximize or minimize respectively. a detailed example of structure of a dataset in MOOT is shown in \ref{}. 
 


\subsubsection{Model Selection}

 
\here{R2c} \BLUE
In this study, our   objective is to compare Synthcore (an LLM-based synthesis method) against   symbolic baselines (UCB, TPE, etc), rather than to benchmark different LLMs against one another. Accordingly, the goal is not to identify which LLM performs best, but to ensure that the LLM used in the evaluation is sufficiently capable to provide a fair and meaningful comparison.

Hence we select Gemini 1.5 Pro, a state-of-the-art model at the time of the study, whose large context window (1M tokens) and strong factual reasoning abilities enable it to handle the rich metadata, multi-example prompts, and high-dimensional structures required for our task. %Using a top-tier model ensures that any observed performance differences reflect methodological distinctions—LLM versus symbolic synthesis—rather than limitations of the underlying LLM.

\BLACK

(Aside: In the prior experiments from \cite{senthilkumar2024can}, both gpt-o1 and gemini-1.5-pro consistently produced similar results for all categories of the datasets (low, medium, \& high dimensional). Therefore,  rather than focusing more on the performance of different models, we focus on comparing LLMs collectively versus other baseline methods. )


For completeness, we  also looked into other models, but
 Gemma-3 and Llama3.1
 have two orders of magnitude fewer variables
than Gemini 1.5 Pro, and they performed poorly on most of the datasets\footnote{This study was conducted before the release of
Gemini 2.5 on March 21, 2025.}.
Also
, DeepSeek-R1 has achieved much recent publicity, but we found
we could not run it locally. Furthermore, the quotes we received for on-line experimentation of that model were prohibitively expensive\footnote{Our experiments require 20 repeats of a 10-way ensemble for 49 data sets. Total quotes
we received from that kind of inference ranged from \$5,000
to \$20,000. Our
lab supports
 a dozen graduate students,
each trying to write, at most, 3 papers per year. Supporting
this kind of inference would
hence cost up to 
$(3*12*20,000)=
\$720,000$, annually.}.
\begin{table}[!t]

\caption{Prompt template for few shot learning, here the two variables are 
{\tt meta} which is a table containing the meta data of the dataset
(name of column, min and max values,
mean or mode, standard deviation or entropy).
{\tt table} which are all the rows
of that dataset.}\label{tbl:fsl}

\centering\footnotesize


\begin{tabular}{|p{0.95\linewidth}|}
\hline

\rowcolor{blue!20} 
\textcolor{black!70}{\textbf{System Message:}} \\
\texttt{You are given a dataset with several features. The rows have been categorized into \textbf{"Best"} and \textbf{"Rest"} examples based on their overall performance. Below are the key features and their descriptions from the dataset:}\\
... \\
\textcolor{black!70}{\texttt{\{rows\_to\_markdown(meta)\}}} \\[1em]

\hline

\rowcolor{teal!15} 
\textcolor{teal!80!black}{\textbf{Human Message:}} \\
\texttt{Given Examples:}\\[1em]
\textcolor{black!70}{\texttt{\{rows\_to\_markdown(table)\}}} \\[1em]

\hline

\rowcolor{gray!15} 
\textcolor{black!80}{\textbf{Task:}} \\
\texttt{Generate an examples that is Better:} \\
\texttt{This should outperform the given \textbf{"Best"} examples by optimizing the relevant features to better combinations.} \\[1em]

\texttt{Consider the inter-dependencies between features, and ensure that the generated examples follow logical consistency within the dataset's context.} \\[1em]

\texttt{Return the output in the same markdown structure:} \\

\hline
\end{tabular}
\end{table}
\subsection{Prompting Strategies with LLMs}\label{prompt}
 In an attempt to emulate human reasoning,
we used LLMs to generate the warm starts needed for
our active learners. These warm starts are created
using the $N$-shot prompt tactic  of Table~\ref{tbl:fsl}.  In summary:
\begin{itemize}
\item The initial {\em System Message}
introduces the LLM to attribute names of a data set. 
\item The subsequent {\em Human Message}
then shows the LLM  $N$ examples, at random, and divide them into half ``best'' and ``rest'' (using Equation~1).
\item The {\em Task} prompt  asks the LLM  to synthesize an example better than all the `best'' and ``rest''  seen so far,   
\item Label and return the row nearest this better synthetic example.
\end{itemize}
We call this an $N$-shot learners where $N$ is the number of
labelled examples given as part of the prompt . For example,
if the {\em Human Message} shows  $N=6$ labelled rows, then we would call that a 6-shot learner.

SynthCore is a 
 \[L + M*N\]
shot  learner where $M$ is the number of
 repeated trials of our $N+1$-shot learners. Unlike Chain of Thought  or MAD   (Multi-Agent   Debates~\cite{smit2023we}), each $M$ trial has no knowledge of the results of the other trials. Rather:
 \begin{itemize}
 \item After labeling $L$  randomly selected rows,
 \item Repeat $M$ times
 \begin{itemize}
 \item Reset the context window of the LLM. 
 \item
 Apply the  $N$-shot learning of Table~\ref{tbl:fsl}.
 In that process, the LLM is given    all the column names and all the x values of the unlabeled rows.
 \end{itemize}
 \item All the $L + M*N$ labels generated by the this loop
 are then sorted via Equation~1.
 \item The best example in that sort is then returned. 
 \end{itemize}
 We recommend this approach for two reasons:
 \begin{enumerate}
 \item
Basic
 $N$-shot learner fails for higher-dimensional optimization
 problems. On the other hand,
 as shown in our {\em Results} section,
 SynthCore's $L+M*N$ approach performs very well,
 \item
 SynthCore is very simple to implement: it is just a wrapper around the established prompt strategy of   Table~\ref{tbl:fsl}.
 \end{enumerate}
Further to the second point,  since SynthCore is so simple,   it is worth asking
 if it merits publication at venues like this journal.
 
 The software engineering and AI literature has many examples where 
something can be very simple, yet still be very significant\footnote{Code reviews  is ``just'' asking that   at least one other developer examines and approves changes before they are merged. Rigby and Bird~\cite{rigby13} demonstrated that this simple addition to a DevOps pipeline yields substantial benefits to organizational knowledge, increasing programmers' understanding of the broader system by 60\% to 150\%. Furthermore, in the projects they studied,  this change transformed code reviews from a defect-finding task into a collaborative problem-solving activity.} \footnote{Boosting is ``just'' resampling technique where learner $i+1$ places increased focus on examples misclassified by learner $i$~\cite{FREUND1997119}. Its co-inventor, Yoav Freund, famously remarked:
{\em Boosting is the best 20-line shell script ever written}.
The AdaBoost script, while very short,  catalyzed a revolution in machine learning. With minimal logic, AdaBoost demonstrated how to transform weak learners into strong ones, laying the groundwork for modern ensemble methods such as XGBoost and LightGBM.}.
While a research {\em result} might be quite simple,
the research {\em effort} to achieve that result can be considerable.
SynthCore's  current   simplicity is the result of months of systematic exploration and overcoming obstacles:
\begin{itemize}
\item Our initial results from 
Table~\ref{tbl:fsl} where obtained using $N=4$ examples. Progressively increasing $N$  did not yield the desired performance gain. Furthermore, since the LLM had to reflect on all rows at each step of its {\em Task}, this approach became
excessively slow.
\item Chain-of-thought (CoT) prompting was investigated, which typically requires the LLM to generate step-by-step reasoning. However, developing an explanation module that allowed the LLM to offer genuinely insightful reflections on our specific tasks proved to be challenging. This experience aligns with findings from other researchers who note that tables can be particularly difficult for models primarily trained on textual sequences, and that complex reasoning over them remains a hurdle even with techniques like normalization~\cite{pourreza2023normtab}.
\item We also explored Multi-Agent Debates (MAD)~\cite{smit2023we}, but the computational costs associated with running multiple agents were prohibitive.
\begin{itemize}
\item
Our experiments were applied to 49 data sets.
\item
In each experiment, $M*N$ times, an LLM has to reflect
on the x-values on the unlabeled rows. As shown in Figure~\ref{dataset}, the MOOT problems can range up to 90,000 rows.
\item
To ensure statistical validity and account for the non-deterministic behavior of the LLM, all our   experiments were repeated 20 times using different random seeds.
\end{itemize}
% Let the budge $B=L+M*(N+1)$.
% Evaluating prompting strategies involved a wide range of labeling budgets $(20\le B \le 100)$.
% For each budget, a portion (say, up to $0.6×B$) was used for initial random labeling, followed by few-shot learning to generate the remaining labels. This amounted to generating
% \[ \sum_{B \in {20,30,\dots,90,100}} (B - 0.6 \times B) = \sum_{B \in {20,30,\dots,90,100}} (0.4 \times B) = 216 \]
% new labels across the budget spectrum for each of the 49 case studies. This process required the LLM to process label definitions and extensive unlabeled data, which was computationally intensive, especially as some datasets (Table~\ref{dataset}) contain over 10,000 rows. 
\end{itemize}
In the end what lead us to SynthCore was the observation
was that through all the above, our methods did sometimes stumble on the optimal solution\footnote{Given data like Table~\ref{dataset}, a solution is ``optimal''  if it is the row closest to the ideal point, as judged by Equation~1.}.
This observation called to mind all the research on ensemble
methods   which lead us to try
SynthCore's  $L+M*N$ ensemble approach.

Before continuing, we make one technical point. After labeling $N$ items, the $N$-shot learner has to
label one more item (the best example seen so far).
Accordingly, the total number of labels required for these methods
is $L+M*(N+1)$.

% \begin{figure}
% \centering
% \includegraphics[width=0.7\linewidth]{LaTeX_DL_468198_240419/Ensemble.png}
% \caption{Ensemble learning using Few-shot data generation method}
% \label{fig:ensemble learner}
% \end{figure}
 

\subsection{Experimental Rig}
\BLUE
\here{R4d} Our choices of the initial annotation budget $L$ and the later sampling budget $M$ follow directly from the Near Enough Optimization (NEO) framework. NEO adopts the view that (a) ``near enough is good enough,'' and (b) solutions within a small effect-size $\varepsilon$ of the optimum are indistinguishable. Under this assumption, sampling is not required to locate the global optimum, but only to encounter \emph{any} solution lying within an $\varepsilon$-neighbourhood of it.

If solutions are randomly shuffled in the dataset (Assumption~1) and values within $\varepsilon$ are indistinguishable (Assumption~2), then the probability of drawing an $\varepsilon$-optimal solution in a single trial is proportional to $\varepsilon$. The waiting time to the first success can therefore be modelled as a geometric process~\cite{ross2014introduction}. After $n$ draws, the probability of having observed at least one success is:
\[
1 - (1-\varepsilon)^n \ge C.
\]

Solving for $n$ yields:
\[
n(C,\varepsilon) = \frac{\log(1-C)}{\log(1-\varepsilon)}.
\]
Cohen's rule~\cite{cohen2016power} is that results are statistically distinguishable  by more than a small effect if they different by half a standard deviation. Assuming a normal distribution with the span
$\pm 3$ then  $\varepsilon=0.5/6$. 
Hence, to be 95\% sure that we have
found solutions that are 
 indistinguishable  from the best, we need the following number of samples:
\[
n(C=0.95) \approx 60
\]
The NEO analysis indicates that $\approx60$ samples are sufficient to reach an $\varepsilon$-near-optimal region. However, once the initial $L$ samples anchor the search, the remaining adaptive rounds need not individually reach the full theoretical requirement. Instead, they serve as incremental refinement steps.

We therefore select $M = 20$ \& $M=30$ as a deliberate engineering compromise:
\begin{itemize}
    \item $M=10$ was too small to provide meaningful refinement in most datasets.
    \item $M=40$ offered marginal improvements while increasing labeling costs.
    \item $M=20$ \& $M=30$ consistently achieved the best trade-off between performance and annotation expense, whereas $M=20$ performed better for low-dimensionality problems, while $M=30$ performed better in medium and high-dimensionality problems.
\end{itemize}

Using the nomenclature from the previous section, we say:
\begin{itemize}
\item $B=L+M*(N+1)$
\item $L=0.6\*B$
\item $M=20$ i.e., perform 20 repeats to test whether our results hold across a range of randomly selected initial conditions.
\end{itemize}

\BLACK




We ran all our active learners with budgets in the range of {20..100} for 20 repetitions to obtain the statistics for the Scott-Knott test. Budgets up to 100 were chosen since, as seen below, no performance improvements were observed over 50 samples.




\subsection{Statistical Methods (Scott-Knott)}\label{stats}

  We run Scott-Knott tests for all the datasets across different treatments, summarize the results in the above subsections, and report the percentage of times a treatment (e.g., "SynthCore") appears in rank $|n|$.  


 Results were ranked  using a combination of the Scott-Knott,
 bootstrap, and Cliff's Delta procedure~\cite{SK74}. 
Scott-Knott recursively partitions sorted treatment means (derived from their performance metric, e.g., Chebyshev distances from the repeats) into statistically distinct, non-overlapping groups \cite{GB91}. Each partition maximizes the between-group sum of squares:
\begin{equation}
SS_B = n_1(\bar{x}_1 - \bar{x})^2 + n_2(\bar{x}_2 - \bar{x})^2
\label{eq:ssb_concise}
\end{equation}
where $n_1, n_2$ are sub-group sizes with means $\bar{x}_1, \bar{x}_2$, and $\bar{x}$ is the combined group mean. 
A key advantage of the Scott-Knott method is that it creates statistically distinct, non-overlapping groups of means. This simplifies interpretation, as there is no ambiguity about which group has what treatment.

Scott-Knott recursively splits treatments   if   a significance test and an effect size test report that     groups are (a) statistically distinguishable and (b) different by more than a small effect.
Cliff's Delta ($\delta$) \cite{Cli93} is a non-parametric effect size that quantifies the magnitude of the difference between treatment performance distributions. It measures the probability of one distribution being stochastically greater than another~\cite{Cli93}.

Bootstrap resampling \cite{Efr79} assesses the statistical significance of the observed differences. This non-parametric technique creates empirical sampling distributions by resampling with replacement from each treatment's 20 performance values. It tests if the observed deviations from random variation are significant, which is valuable for hypothesis testing and model validation, particularly with limited or non-normally distributed data \cite{ET93}.


 
 

\section{Results}\label{results}

Before conducting comparative evaluations of different methods, it is prudent to first document the baseline performance of our proposed approach. Why?  Reporting only relative results, without first establishing an absolute baseline,   undermines the significance of the results and should be avoided.
All too often, we read research papers where authors   overlook this step and then report seemingly impressive improvements—such as a X\% gain— which is
 misleading when the baseline itself is unacceptably low. 
 
 \begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{chart-all.png}
    \caption{Performance of Synthcore 20, 
    (shown in red) fall very close to the
    best possible optimal (shown in green).}
    \label{lobaseline}
\end{figure}


 Figure~\ref{lobaseline} shows
 mean results from twenty runs of Synthcore (budget $B=20$ labels, LLM‐generated warm start):
\bi
\item
The 
green curve sorts data by their best row that is closest to “heaven” (see Equation \ref{eq:ch}).
\item
The black curve represents the untreated ``before'' condition of our data. This curve shows the  average distance to heaven if all rows.
If we just picked random
rows, we would usually achieve the results shown in black.
\item 
The red curve shows the rows
found by Synthcore.
\ei
By our design, every optimization falls between the black (mean) and green (minimum) lines.
Hence, the closer
the red curve falls to the green curve, the better the performance. The red curve’s proximity to green shows that Synthcore delivers strong results.
 

(Aside: not shown in Figure~\ref{lobaseline} are
statistical tests that rank Synthcore's performance against other   state-of-the-art methods. For those statistical comparisons, see below.)



\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{performance_3.png}
    \caption{Performance of different active learners w.r.to budgets. Best is the average heaven values for all the datasets}% green line "lo (baseline) % yaxm=0.6
    \label{budget-vs-heaven}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{runtimes_3.png}
    \caption{Average Run times of different treatments, y axis is time in seconds scaled to logarithmic values and x axis is budget ranging from 20 to 100}% change y range 0.6 max" %cahnge x axis label to "Budget"
    \label{runtimes}
\end{figure}



Figure~\ref{lobaseline} showed   Synthcore's benefits. Those benefits came at some cost:
\bi
\item
The cost of collecting labels;
\item
The CPU runtime cost.
\ei
  Recall that the Figure~\ref{lobaseline}   achieved at a cost of just a few labels ($B=20$). 
  Using 
   Figure~\ref{budget-vs-heaven} and Figure~\ref{runtimes}
 we can now justify that budget:
 \bi
 \item
 Figure~\ref{budget-vs-heaven}
 shows mean distance to heaven across all data sets in twenty repeated trials of our different optimizers.
  The bottom curve shows  the mean distance of heaven of the best row in each data set. 
 The x-axis shows how performance changes as the budget increases. The
 statistical tests of \S\ref{stats}
 shows no statistical different between the $B=20,30,50,100$ results.
\item
Figure~\ref{runtimes} shows the runtimes associated with budget sizes associated with varying budgets.
Comparing budgets of 20 and 100,
we note that Synthcore is much slower for larger budgets; e.g.
at $B=1000$ it is five times slower than at $B=20$.
 \ei
In summary, the (a)~runtimes are faster at smaller budgets;
and (b)~there is no 
 statistically significant optimization improvement at larger budgets. Accordingly,
 the rest of out results will be reported using $B=20$.

 Before continuing, we answer one frequently asked question: Why the orders of difference in run times between the lower and upper curved of Figure~\ref{runtimes}?
 The two lower curves from simple Bayesian methods that can update their models for every row,
 in linear time. The upper 
 two curves come from methods that are far more reflective. Specifically,
 the upper methods made decision by   (a)~reflecting over  a large space of possible models (UCB\_GPM),
 or (b)~reflecting over a large context window that struggled to pay
  attention to hundreds to thousands of rows (SythnCore). Hence it is hardly surprising
 that that more reflective methods (UCB\_GPM and SynthCore) are far slower than the other methods.
 


\subsection{Answers to Research Questions}
   

Have documented the absolute performance
of SynthCore, we now move to statistical
comparisons between different methods.



These comparisons are shown in Tables~\ref{lo0}, \ref{med0}, \ref{hi0} and 
Tables~\ref{lo}, \ref{med}, \ref{hi}
Each cell in this table shows the percentage of times (in 20 repeats) that a particular treatment was ranked best (i.e.  rank=0), or some other rank, by the statistical methods
of \S\ref{stats}. The most important part of these tables are the rank=0 results. For example, top left of
 Table~\ref{lo0}, we see that 100\% of the time, an LLM method achieved a top rank.



 The red results in Tables~\ref{lo0}, \ref{med0}, \ref{hi0} come from prior work 
 by
 Senthilkumar et al.~\cite{senthilkumar2024can}
 that ran the few-shot procedure of
 Table~\ref{tbl:fsl}, only once (no ensembles).
 Here we see the negative prior result that inspired this  study:
 \bi
 \item
 Tables~\ref{lo0}, \ref{med0} that LLM warm start method did as well, or better than anything else.
 \item
  But in Table~\ref{hi0}, LLMs were only half as good as the best method (UCB\_GPM).
 \ei
 The blue results in 
 Tables~\ref{lo}, \ref{med}, \ref{hi}
 show the performance improvements achieved by SynthCore's
 ensemble methods. Across all our data,
 SynthCore performed better than everything
 else. Hence we say:

{\textbf{RQ1:} {\em{Is ensemble few shot learning better than standard few shot learning for SE Active Learning?}}  
Answer: {\bf Yes}. 



\begin{adjustbox}{scale=1}
 
 
 \begin{minipage}[t]{0.5\textwidth}
% SECOND COLUMN TABLE 1

 

\begin{table}[H]
\centering
\scriptsize

{\bf Column 1 \\ Prior results, from~\cite{senthilkumar2024can}. \\  (no ensembles).}
\vspace{5mm}

\begin{tabular}{c|c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c} 
&\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\

  & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
LLM Warms  & \colorbox{darkred}{\makebox[0.4cm][l]{\textbf{100}}} &  & &  &  &  &  \\
Exploit  & \colorbox{red}{\makebox[0.3cm][l]{73}} & 18 & 9 & & & &  \\
UCB\_GPM & \colorbox{red}{\makebox[0.3cm][l]{55}} & \colorbox{lightred}{\makebox[0.3cm][l]{27}} &  & 9& &   \\
TPE & \colorbox{lightred}{\makebox[0.3cm][l]{27}} & \colorbox{lightred}{\makebox[0.3cm][l]{27}} & & 9 & 18 & 9\\
Explore   &   9 & 18 & 18 & \colorbox{lightred}{\makebox[0.3cm][l]{27}}& & &  \\
random    & \colorbox{red}{\makebox[0.3cm][l]{36}} &  & 9 & 9 & 9 & 9 \\
Baseline  & & & & & 9 &\colorbox{lightred}{\makebox[0.3cm][l]{18}} &  \\
\hline
\end{tabular}
\caption{Low dimensional   ($x < 6$).}\label{lo0}
\end{table}

% Add vertical spacing between tables 5 and 6


% SECOND COLUMN TABLE 2
\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{c|c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c}  
&\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\

  & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline

UCB\_GPM & \colorbox{darkred}{\makebox[0.3cm][l]{\textbf{64}}} & 18 & 9 & 9 &  & &9 \\
LLM Warms  & \colorbox{darkred}{\makebox[0.3cm][l]{\textbf{64}}} &  9& 18&  & &  &  \\
Exploit  & \colorbox{red}{\makebox[0.3cm][l]{45}} & 18 & \colorbox{lightred}{\makebox[0.3cm][l]{27}} & 18 &  10 & &  \\
TPE & \colorbox{red}{\makebox[0.3cm][l]{36}} & 18 & 18 & 18 & & 9\\
random    & \colorbox{lightred}{\makebox[0.3cm][l]{27}} & 18 &  \colorbox{lightred}{\makebox[0.3cm][l]{27}}& 18 & &  \\
Explore   &   27 & 9 & 18 & \colorbox{lightred}{\makebox[0.3cm][l]{20}} & & 10& 10 \\
Baseline  & 9 & 9& 9 &  &  & 18 & \\
\hline
\end{tabular}
\caption{Medium dim.   ($5 < x < 11$).}\label{med0}
\end{table}


% SECOND COLUMN TABLE 3
\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{c|c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c} 
&\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\
  & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline

UCB\_GPM & \colorbox{darkred}{\makebox[0.4cm][l]{\textbf{100}}} &  & & &  & &  \\
LLM Warms & \colorbox{red}{\makebox[0.3cm][l]{\textbf{53}}} & 13 & 7& 13 &  &  &  \\
Exploit  & \colorbox{red}{\makebox[0.3cm][l]{\textbf{47}}} & 13 & 13 & 13 &   & &  \\
Explore   & 7 &  & 13 & 20 & 20& &  \\
TPE & \colorbox{red}{\makebox[0.3cm][l]{\textbf{40}}} & 13 & 7 & 13 & 7 & 7 \\

random    & &\colorbox{lightred}{\makebox[0.3cm][l]{27}} & \colorbox{lightred}{\makebox[0.3cm][l]{27}} & 20& 13 & 13 \\
Baseline & &7 & 13&  & 9 & 27 &  \\
\hline
\end{tabular}
\caption{High dimensional   ($x > 10$).}\label{hi0}
\end{table}

\end{minipage}

\hspace{0.01\textwidth} % <-- SMALL SPACE BETWEEN COLUMNS
\begin{minipage}[t]{0.5\textwidth}
% FIRST COLUMN TABLE 1
\begin{table}[H]
\centering
\scriptsize
{\bf Column 2 \\ Our results \\  (with few-shot ensembles).}
\vspace{5mm}

\begin{tabular}{c|c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c} 
&\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\

  & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
SynthCore  & \colorbox{darkblue}{\makebox[0.3cm][l]{\textbf{86}}} & 7  & &  &  &  &  \\
Exploit  & \colorbox{blue}{\makebox[0.3cm][l]{\textbf{71}}} & 7 & 7 & 7 &  7 & &  \\
UCB\_GPM & \colorbox{blue}{\makebox[0.3cm][l]{\textbf{43}}} & \colorbox{lightblue}{\makebox[0.3cm][l]{29}} & 7 & 7& 14 & &  \\
TPE & 7 & \colorbox{lightblue}{\makebox[0.3cm][l]{29}} & 7 &  14&  14 \\
Explore   &   7 & 7 & \colorbox{lightblue}{\makebox[0.3cm][l]{29}} & \colorbox{blue}{\makebox[0.3cm][l]{36}} & 14& 7& 7 \\
random    & 7 & \colorbox{blue}{\makebox[0.3cm][l]{36}} & \colorbox{lightblue}{\makebox[0.3cm][l]{29}} & 14 & 7 & 7 \\
Baseline  & & & & & 9 & \colorbox{lightblue}{\makebox[0.3cm][l]{21}} &  \\
\hline
\end{tabular}
\caption{Low dimensional   ($x < 6$).}\label{lo}
\end{table}
% FIRST COLUMN TABLE 2
 \begin{table}[H]
\centering
\scriptsize
\begin{tabular}{c|c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c} 
&\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\

  & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
SynthCore  & \colorbox{darkblue}{\makebox[0.3cm][l]{\textbf{85}}} &  & &  &  8&  &  \\
UCB\_GPM & \colorbox{blue}{\makebox[0.3cm][l]{\textbf{77}}} & 8 & 8 & &  & & \\
Exploit  & \colorbox{blue}{\makebox[0.3cm][l]{\textbf{69}}} &  & 8 & 8 &  8 & &  \\
TPE & \colorbox{blue}{\makebox[0.3cm][l]{39}} & 8 & 8 & 15& 15 & \\
random    & \colorbox{blue}{\makebox[0.3cm][l]{31}} & 15 &  \colorbox{blue}{\makebox[0.3cm][l]{31}}& 15 & &  \\
Explore   &   9 & 9 & \colorbox{blue}{\makebox[0.3cm][l]{41}} & \colorbox{darkblue}{\makebox[0.3cm][l]{23}} & & 8& 8 \\
Baseline  & 8& & 15& 8& 15 & 15 &  8\\
\hline
\end{tabular}
\caption{Medium dim.   ($5 < x < 11$).}\label{med}
\end{table}

% FIRST COLUMN TABLE 3
\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{c|c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c}
&\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\

  & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
SynthCore  & \colorbox{darkblue}{\makebox[0.3cm][l]{\textbf{71}}} & 5 & 11&  8&  &  &  \\
UCB\_GPM & \colorbox{darkblue}{\makebox[0.3cm][l]{\textbf{68}}} & 11 & 11 & &  & & 5 \\
Explore   & \colorbox{blue}{\makebox[0.3cm][l]{37}} & 11 & \colorbox{lightblue}{\makebox[0.3cm][l]{21}} & 5 & 16& &  \\
TPE &\colorbox{blue}{\makebox[0.3cm][l]{32}} & 16 &16 & 5 & 5 & 5 \\
Exploit  & 16 & 16 & \colorbox{blue}
{\makebox[0.3cm][l]{32}} & 11 &  5 & 5& 5 \\
random    & &\colorbox{lightblue}{\makebox[0.3cm][l]{21}} & \colorbox{lightblue}{\makebox[0.3cm][l]{21}} &  16& 11 & 11 \\
Baseline  & & & 5& 11& 11 & \colorbox{lightblue}{\makebox[0.3cm][l]{26}} & 5 \\
\hline
\end{tabular}
\caption{High dimensional   ($x > 10$).}\label{hi}
\end{table}

\vspace{5mm} 
\end{minipage}


\end{adjustbox}



\newpage
Moving on to our second research question,
this work was motivated by a prior study that reported
    LLMs failed   on
    higher-dimensional. Therefore,
    in our results, we must pay particular
    attention to this kind of data.
 
    
    {\textbf{RQ2:} {\em {Does this improvement hold for high dimensional data?}}  Comparing Table~\ref{hi0} and
    Table~\ref{hi}, we see, for high-dimensional,
      ensembles thrive where solo-based
    reasoning fails. Hence for this question  we answer {\bf Yes}.

    
    
  

 
 
%  that did   used few shot learning, but not  enembles.

% The primary motivation for using an ensemble of few-shot learners over a single few-shot learner is to achieve faster, more diverse, and robust exploration. By leveraging multiple learners, the ensemble can explore the search space more comprehensively, reducing the risk of getting stuck in local optima. As shown in Tables \ref{}, our ensemble methods consistently outperformed both random sampling and Bayesian acquisition strategies (GPM and TPE) across low, medium, and high-dimensional problems, demonstrating their effectiveness in varied scenarios.
% \begin{table}[!b]
% \centering
% \scriptsize
% \begin{tabular}{c|ccccccc}
% \hline
% &\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\
% \hline
% Acquire & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
% \hline
% SynthCore  & \cellcolor{darkblue}\textbf{86} & 7  & &  &  &  &  \\
% Exploit  & \cellcolor{blue}\textbf{71} & 7 & 7 & 7 &  7 & &  \\
% UCB\_GPM & \cellcolor{blue}\textbf{43} & \cellcolor{lightblue}29 & 7 & 7& \cellcolor{lightblue}14 & &  \\




% Explore   &   9 & 9 & \cellcolor{lightblue}29 & \cellcolor{blue}36 & 14& 7& 7 \\
% random    & 7 & \cellcolor{blue}36 & \cellcolor{lightblue}29 & 14 & 7 & 7 \\

% %rrp  & & 9& & 9& 9 & \cellcolor{lightblue}18 & \cellcolor{lightblue}18 \\
% Baseline  & & & & & 9 & \cellcolor{lightblue}21 &  \\
% \hline
% \end{tabular}
% \caption{Percent frequency for some treatment appearing at some rank. e.g. ``50'' means that a treatment achieved a rank in half of our 49 data sets.  Results from 12 low dimensional  data sets (i.e. with   $x< 6$  independent features).}\label{lo}
% \end{table}

% \begin{table}[!b]
% \centering
% \scriptsize
% \begin{tabular}{c|ccccccc}
% \hline
% &\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\
% \hline
% Acquire & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
% \hline
% SynthCore  & \cellcolor{darkblue}\textbf{85} &  & &  &  8&  &  \\
% UCB\_GPM & \cellcolor{blue}\textbf{77} & 8 & 8 & &  & & \\

% Exploit  & \cellcolor{blue}\textbf{69} &  & 8 & 8 &  8 & &  \\

% random    & \cellcolor{blue}31 & 15 &  \cellcolor{blue}31& 15 & &  \\
% Explore   &   9 & 9 & \cellcolor{blue}46 & \cellcolor{lightblue}23 & & 8& 8 \\


% %rrp  & & 9& & 9& 9 & \cellcolor{lightblue}18 & \cellcolor{lightblue}18 \\
% Baseline  & 8& & 15& 8& 15 & 15 &  8\\
% \hline
% \end{tabular}
% \caption{Frequency of ranks achieved.
% Same format as Table~\ref{lo}.  For 16 medium dimensional   datasets (with $5< x < 11$   features).}\label{med}
% \end{table}

% \begin{table}[!b]
% \centering
% \scriptsize
% \begin{tabular}{c|ccccccc}
% \hline
% &\multicolumn{7}{c}{\textbf{Scott-Knott Rankings}}\\
% \hline
% Acquire & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
% \hline
% SynthCore  & \cellcolor{darkblue}\textbf{71} & 5 & 11&  8&  &  &  \\
% UCB\_GPM & \cellcolor{darkblue}\textbf{68} & 11 & 11 & &  & & 5 \\
% Explore   &  \cellcolor{blue}37 & 11 & \cellcolor{lightblue}21 & 5 & 16& &  \\
% Exploit  & 16 & 16 & \cellcolor{blue}32 & 11 &  5 & 5& 5 \\



% random    & &\cellcolor{lightblue}21 & \cellcolor{lightblue}21 &  16& 11 & 11 \\

% %rrp  & & 9& & 9& 9 & \cellcolor{lightblue}18 & \cellcolor{lightblue}18 \\
% Baseline  & & & 5& 11& 11 & \cellcolor{lightblue}26 & 5 \\
% \hline
% \end{tabular}
% \caption{Frequency of ranks achieved.
% Same format as Table~\ref{lo}.  For 19 high dimensional   datasets (with $x>10$ independent features).}\label{hi}
% \end{table}
 
 To say all this another way,
 for this kind of optimization, LLMs need to be encouraged to divulge their knowledge. 
 A single prompt is not enough, nor is simple few shot learning. Rather, it is needed to run multiple sessions with an LLM, then aggregate the results.


\section{Discussion}

SynthCore demonstrates that large language models, though 
unreliable as single annotators, can become powerful contributors 
when treated as ensembles of weak, diverse learners. While past 
studies have shown that few-shot prompts struggle in 
high-dimensional SE tasks, our results indicate that diversity in 
prompting can significantly mitigate this limitation. Across 49 
datasets, SynthCore achieved consistent gains over traditional 
optimization baselines and even surpassed adaptive Bayesian 
learners in many cases.



One reason SynthCore works is that ensemble prompting reduces 
overfitting to prompt phrasing or context. Each prompt acts as a 
sample from the LLM’s latent capability space, and their 
aggregation offers resilience against individual prompt 
failures. This is especially valuable in SE optimization, where 
initial warm-starts are crucial to guiding the search process. 
Rather than placing all trust in a single LLM prediction, 
SynthCore leverages variation to increase coverage of plausible 
solutions early in the learning loop.





However, the benefit of ensembling is not uniform across all tasks.
Our experiments show that ensemble gains are greatest in 
moderate-dimensional spaces with clear trade-offs, such as 
project planning or Makefile tuning. In simpler tasks, single-shot 
learners often suffice. Few-shot prompting typically shows the 
highest returns when moving from 0 to 1 shot, with diminishing 
returns after 4 shots. Ensembles counteract this drop-off, 
particularly in higher-dimensional settings.

Crucially, SynthCore avoids complex multi-agent orchestration. 
Unlike chain-of-thought prompting or debate-style sampling, it 
does not rely on model introspection or self-consistency checking.
Instead, it uses independent prompts with no cross-talk. This 
simplicity makes SynthCore faster to run, easier to adapt to new 
domains, and more robust to failures in reasoning depth or 
hallucinated explanations. In practice, this makes SynthCore more 
applicable in real-world SE pipelines where latency and 
predictability matter.

\here{R3d}
\BLUE
For practitioners, we recommend using SynthCore following the workflow shown in Figure~\ref{recommend}. Our results indicate that SynthCore offers the largest performance gains in complex, high-dimensional SE optimization tasks where human heuristics are difficult to encode. For SE optimization problems, we advise scaling the evaluation budget with the complexity of the data—using the number of independent variables as a practical indicator—and increasing SynthCore’s budget accordingly. However, SynthCore is not universally optimal. When strong SME expertise is readily available, relying on a human SME may be more cost-effective than invoking LLM ensembles. Likewise, for highly structured and common tasks such as sentiment analysis or bug detection, standard LLM prompting already performs well, and the additional API cost and runtime of ensemble prompting may not be justified.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Recommendation.png}
    \caption{\protect\here{R3d-2} Recommendations to practitioners.}
    \label{recommend}
\end{figure}

\BLACK

SynthCore is also efficient. Compared to multi-stage LLM pipelines 
that require dialog, argument tracking, or self-refinement, 
SynthCore relies on multiple single-prompt calls—each simple, 
stateless, and fast. This architecture is well-suited to 
deployment within CI/CD workflows or as part of configuration 
tuning tools. Our results show that ensembles of size 3–5 provide 
most of the performance gains, making this approach viable even 
under strict budget constraints.

Beyond performance, SynthCore also surfaces a deeper point about 
LLM limitations. Prior work (e.g., Ahmed et al. at MSR 2025) has 
shown that LLM agreement with human annotators is not always a 
good proxy for truth. Our work shifts the question: rather than 
ask whether an LLM is “correct,” we ask whether a group of diverse 
LLM prompts can generate a fertile search space. Seen in this way, 
LLMs are not oracles—they are generative seeds for optimization.

\BLUE

\here{R3g}  More generally, we say that SynthCore functions as a subject matter expert amplifier, not a replacement. Its primary role in the pipeline is to compress the human expert's review surface. Instead of requiring an SME to laboriously generate and evaluate a large set of candidates---many of which are suboptimal---SynthCore filters and surfaces a small, high-quality candidate set for validation. This fundamentally shifts the SME's task from slow, exhaustive \emph{generation} to rapid, high-leverage \emph{validation}.



This transformation provides concrete benefits for efficiency and iteration speed. Without SynthCore, an SME might label over 100 examples to identify the 5 truly optimal configurations. With SynthCore, the SME's effort is focused on reviewing approximately 20 top candidates, validating, or making minor adjustments. This reduces the SME's  cognitive load significantly, as evaluating five promising designs is an intrinsically different task from shifting through fifty unknowns. Furthermore, human judgment is thus applied precisely where it is most needed: assessing ambiguous trade-offs, applying domain constraints the LLM may not possess, and integrating specific organizational context.


Regarding the need for human SME input, SynthCore reduces the required volume of input for a given quality threshold. It facilitates the democratization of the initial design phase. Junior engineers, leveraging SynthCore, can rapidly produce reasonable first drafts and solution proposals that previously required the intervention of a senior SME. Senior SMEs can then dedicate their limited, high-value time not to initial exploration, but to final refinement and tackling complex, boundary-pushing engineering challenges.



\here{R3d-b} However, we do not claim SynthCore to be a universal solution for annotation in multi-objective optimization problems. While text and code are indeed high-dimensional modalities, modern LLMs handle them remarkably well due to their training objectives and architectural design. This is also reflected in the tasks explored by Ahmed et al., which were predominantly text- and code-centric (e.g., sentiment classification), where LLMs naturally excel.
In contrast, our focus is on tabular multi-objective optimization problems, a setting where LLMs typically struggle. Tabular data lacks the sequential structure and semantic regularities that LLMs exploit in language or code, making annotation and performance estimation significantly more challenging. For this reason, SynthCore is specifically designed for such structured, non-textual domains.

At the same time, we acknowledge that SynthCore may not generalize well to multimodal, ultra–high-dimensional domains such as audio or images, which fall outside the native capabilities of current LLMs. Addressing these more complex modalities would likely require future extensions that incorporate multimodal foundation models \cite{wang2023visionllm} capable of reasoning jointly over text, vision, and other signals.

\BLACK

\subsection{Future Work}

We plan to explore transferability across iterations. Currently, 
SynthCore treats each ensemble as independent, but a sequential 
execution model—where insights from early prompts inform later 
ones—could reduce sampling cost while improving guidance. This 
strategy may enable higher efficiency with fewer prompt calls.

A natural next application is Next-Release Planning (NACP), where 
trade-offs among value, effort, and delivery constraints align 
with SynthCore's strengths in navigating sparse, 
multi-objective spaces. Here, ensembles could support decisions 
under uncertainty with little labeled data.

Another direction is extending SynthCore to structured domains 
such as patch generation, bug triage, or Makefile tuning. These 
require prompts that account for semantics and ordering. Ensemble 
diversity may help span different reasoning paths over these 
structures.

We also intend to benchmark SynthCore against expert-curated 
annotations—not just to evaluate predictive accuracy, but to 
assess alignment with human judgment and trust. This could 
clarify the role of LLMs as surrogates or collaborators.

Explainability remains a key open challenge. While SynthCore 
improves sampling, it does not make LLM reasoning more transparent.
Post-hoc interpretation of prompt choices or traceable 
rationales could make the system more suitable for regulated or 
high-stakes domains.

Finally, we plan to automate prompt selection using clustering or 
meta-learning. Rather than relying on static prompt sets, 
SynthCore could dynamically adjust ensemble makeup based on task 
feedback or historical performance, further improving generality 
and sample efficiency.

\subsection{Threats to Validity}

\paragraph{Construct Validity.} While our evaluations use standard 
multi-objective metrics, these may not capture qualities important 
to practitioners, such as long-term maintainability or developer 
trust. Further studies could compare SynthCore-generated configs 
against expert-labeled benchmarks or measure downstream effects on 
developer workflow.

\paragraph{Internal Validity.} Though care was taken to verify our 
implementations, there may be latent bugs or assumptions in our 
active learning and acquisition functions. Hyperparameters for 
GPM, TPE, and LLM prompting were chosen empirically, and small 
changes may affect outcomes. We encourage replication and 
sensitivity testing using our reproduction package.

\paragraph{External Validity.} Our 49 datasets cover a broad range 
of SE optimization problems, but not all real-world tasks. Tasks 
with strong semantic constraints—like patch generation or 
prioritization based on security context—may demand additional 
domain knowledge or hybrid LLM-human annotation schemes. Also, 
while we used open-source models, performance may differ for 
closed commercial APIs.

\section{Conclusion}

This work presents SynthCore, a simple but powerful ensemble 
prompting technique for LLM-based annotation and optimization in 
software engineering. By combining multiple few-shot prompts with 
no coordination or chain-of-thought overhead, SynthCore achieves 
state-of-the-art performance on high-dimensional SE optimization 
tasks—outperforming Bayesian methods, active learners, and 
single-prompt baselines. All results were achieved using only 
LLM-generated data, without human intervention.

Our findings reframe how LLMs should be used in SE. Rather than 
expecting correctness from a single prompt, we should treat LLMs 
as stochastic samplers of reasoning fragments. When these samples 
are aggregated via SynthCore, they yield diverse and effective 
search spaces for optimization—even on tasks where single prompts 
fail.

SynthCore’s value lies in its generality. It requires no 
fine-tuning, no deep prompt engineering, and no supervision. It is 
easy to implement, extensible to many task types, and efficient 
enough to use in budget-constrained settings. This makes it a 
practical building block for next-generation SE tooling.


 \newpage
 \newpage
\section*{Declarations}

\begin{description}
\item[\bf Source of Funding: ] \hfill \\
No funding bodies were involved in the creation
of this work.
\item[\bf Financial or non-financial interests: ] \hfill \\
The authors have no relevant financial or non-financial interests to disclose.
All authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest or non-financial interest in the subject matter or materials discussed in this manuscript.

\item[\bf Ethical approval:] \hfill \\
Lacking human or animal subjects,
an ethical review was not required. 
\item[\bf Informed consent:] \hfill \\
This study had no human subjects.
\item[\bf Author Contributions:] \hfill \\
All this paper's experimental work was conducted by Lohith Senthilkumar.
Lohith Senthilkumar and
Tim Menzies contributed equally
to the writing of this paper. 
\item[\bf Data Availability Statement:] \hfill \\
To repeat and/or refine and/or refute this work, see our scripts and data at
\url{https://github.com/lohithsowmiyan/lazy-llm/tree/clusters}.
\item[\bf Conflict of Interest:] \hfill \\ The authors declared that they have no conflict of interest.
The authors have no competing interests to declare that are relevant to the content of this article.
\item[\bf Clinical Trial Number:] \hfill \\
Clinical trial number: not applicable.
\end{description}
 \newpage



 



%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements} 


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

% Non-BibTeX users please use
\bibliographystyle{plain} % Choose a style (e.g., plain, unsrt, alpha, etc.)
\bibliography{123} % name,timm,sample-base} %

\newpage

\subsection*{Response to Reviewers for EMSE-D-25-00425}


We thank the editors and reviewers for their thoughtful, constructive, and detailed feedback. We have carefully revised the paper in response to all comments. Below, we outline the major changes made, organized by the source of feedback. For each comment, we restate the original point and describe how we addressed it.


\subsection*{Comments for the authors}
This paper is an interesting idea which aims at important research field. However, there are several issues in this paper that need to be improved.

\begin{enumerate}[label=\normalfont{Comment \arabic*:}, leftmargin=7.5em]
\item \normalfont{The evaluation is limited. Many LLMs are mentioned without evaluation.}

\BLUE
Thank your for that note.
Please forgive us, that was our fault, we were not clear on the overall goals of those paper.  This study does not attempt to determine which specific LLM architecture is
optimal for optimization tasks; that question remains for future work. Instead,
it establishes a clear empirical result: \textbf{LLMs are highly effective
warm-start mechanisms for software engineering optimization}, outperforming
long-standing symbolic baselines even in high-dimensional settings. This
contribution helps fill a documented gap in the SE literature, where only
$\sim$5\% of LLM papers compare against non-LLM approaches~\cite{Hou24}, and
where several recent studies report pessimistic assessments of LLM performance.
Our findings provide robust evidence to the contrary, showing that LLMs deliver
a substantial and practical advantage when initializing SE optimization
workflows.
 
We have added this note to the paper \there{R0a} and
\there{R2c}.
\BLACK

\item \normalfont{Lack of context and background. Much information is not introduced very well.}

\BLUE
Thank you for that note. We agree with you that
this paper needs much careful expansion. As shown in the BLUE text throughout this revision,
this draft contains many expanded points.
\BLACK


\item \normalfont{Value and highlight potential limitations of the approach are not discussed.}

\BLUE
You make a good point. Please see our new discussion section where these matters are reviewed.
\BLACK


\end{enumerate}

\subsection*{Response to Reviewer 1}
We received no comments from reviewer1.

\subsection*{Response to Reviewer 2}

\begin{enumerate}[label=\normalfont{Comment \arabic*:}, leftmargin=7.5em]
    \item \normalfont{While the paper addresses an important challenge in SE optimization and presents extensive experiments, several serious issues limit its contribution in its current form.

The proposed SynthCore is essentially a repetition of a fixed prompt template with multiple independent runs and aggregation of outputs. This is conceptually very close to established methods such as Self-Consistency prompting and other voting-based ensembles. The paper does not provide a theoretical rationale for why SynthCore is fundamentally different, nor does it include empirical comparisons against these related baselines.
}\\

\BLUE
Thank you for the comment. In the previous draft, we did not clearly articulate how our approach differs from self-consistency prompting. Now, in \there{R2a-1} \& \there{R2a-2} 9and Table~\ref{consistent} we provide a detailed explanation of these distinctions and clarify why SynthCore is fundamentally different both conceptually and operationally.


\BLACK

\item \normalfont{
All experiments are conducted on MOOT-like tabular datasets involving multi-objective optimization problems. While these datasets are relevant to certain SE optimization tasks, they do not cover the diversity of labeling problems in software engineering. This narrow scope limits how far
we can generalize of findings and weakens the broader impact of the proposed method.

}

\BLUE
Please excuse us, we do not mean to be argumentative,
but ``just'' restricting the result to tabular higher dimensional multi-objective problems
actually covers ain interesting   space of problems. While
working on this revision, we served as 
ICSE'26 and FSE'26 reviewers. In
that work, we were assigned four ICSE'26 and  FSE'26 submissions that
all used a small subset of the data explored here. Those
papers explored ten data sets, we explore 49. Which is to say that while we ``just'' explore optimization,
in that sub-arena of SE, our experiments are far more
extensive than very recent state-of-the-art submissions
to leading venues. 

Also, we argue that the analysis of tabular
data is not simplistic or outdated.  Tabular data is widely used. Somvanshi et al. (https://arxiv.org/abs/2410.12034) call it “the most commonly used data format
in many industries...finance, and transportation.”. Commercial use in data synthesis/privatization
for health/government (https://ieeexplore.ieee.org/abstract/document/10438420)  GitHub-scale code analytics uses tabular data (e.g., CommitGuru5,
used in many SE papers). Also, returning to those ICSE'26 and FSE'26 papers we mentioned above, all those papers use case studies expressed  as tabular data.

Further, the  processing of tabular data is challenging, even for LLMs. Somvanshi et al. (https://arxiv.org/abs/2410.12034)
report that “despite deep learning’s success in image and text domains, tree-based models like 
XGBoost and Random Forests continue to outperform neural networks on medium-sized tabular
datasets. This performance gap persists even after extensive hyperparameter tuning."
\BLACK

\item \normalfont{
 The evaluation is dominated by results from a single LLM (i.e., Gemini 1.5 Pro). Although other models (e.g., GPT, LLaMA) are briefly mentioned, their results are neither systematically evaluated nor reported in detail. This raises concerns about whether the proposed approach is specific to one model's capabilities. Cross-model experiments are essential to demonstrate that the approach is robust across different LLMs.
}

\BLUE 
Thank you for the comment. We agree with your observation and apologize for not making our objective clear in the previous draft. Our goal is not to benchmark the performance of different LLMs for this task, but rather to evaluate how well our LLM-based method, SynthCore, compares against symbolic baselines. To ensure a fair comparison, we selected Gemini 1.5 Pro, a sufficiently capable state-of-the-art model at the time of the study. We have clarified this study design rationale in the revised manuscript at \there{R0a} and \there{R2c}.
\BLACK

\item \normalfont{ The related work section is disproportionately long and includes extensive repetition of prior results, which could be summarized more concisely. This pushes the presentation of the method to page 12 of a 25-page body, which makes it difficult for readers to quickly grasp the main technical contribution.
}

\BLUE
We agree, the old paper was too
verbose. Here, we have reduced that 8 page section to under 3. Which means our method is introduced far earlier in the paper. 
\BLACK

\item \normalfont{The results section presents many tables and statistical rankings, but the key takeaways are not clearly distilled. For example, while the data might indicate that SynthCore improves performance in high-dimensional settings, this is not clearly summarized in prose with supporting evidence and effect sizes.

}

\BLUE 

Thank you for the comment. We agree that the previous draft did not sufficiently discuss the implications of our results. In this revision, we have added a dedicated recommendation for practitioners (see \there{R3d}). These recommendations clarify when our methods should or should not be used, given constraints such as available budget, task characteristics, and data dimensionality. We believe these additions provide practical guidance for industry practitioners working on configuration tasks.
\BLACK


\item \normalfont{Other minor issues:
Page 3, Line 12, a reference is missing
Page 9, Referenced another Table (i.e., Table 2) in Table 1}
\BLUE 
Thank you for the comment. We have fixed the above issues

\BLACK 
 


\end{enumerate}


\subsection*{Response to Reviewer 3}

\begin{enumerate}[label=\normalfont{Comment \arabic*:}, leftmargin=7.5em]

\item \normalfont{The topic of the paper is highly relevant to both empirical SE and AI-for-SE. The experimental setup is rigorous. The authors compare SynthCore to strong baselines (Gaussian Process Models, Tree of Parzen Estimators, standard few-shot LLMs), and the results are convincing when it comes to demonstrating performance improvements in high-dimensional tasks. The approach is methodologically solid. SynthCore is simple yet effective, and relies on well-motivated ensemble learning principles and validated through statistical analysis (Scott-Knott, bootstrapping, Cliff's Delta). Moreover, the authors were good at motivating their approach throughout the paper and the relevance of their methods despite its simplicity. }

\BLUE

Thank you for those kind words

\BLACK
\item \normalfont{The introduction does not prepare the reader for the core problem space of the paper. Challenges of annotation with LLMs are outlined, but the central focus of the paper: multi-objective optimization, is only briefly mentioned. The authors should better motivate why optimization in SE is important.}
 

\BLUE
Thank you for that comment. You are quite correct, we needed to add more material in that area. Please see \there{R3c}.

\BLACK

\item \normalfont{
Many critical concepts (multi-objective reasoning, active learning, warm-starts) are only introduced in depth in the related work section. This makes the paper hard to follow, especially for readers that are unfamiliar with the optimization literature. These concepts should be introduced earlier in the paper with concrete examples to help contextualize the contribution.
}

\BLUE
 Thank your for emphasizing this point. As mentioned above, we have added brief explanations for the above concepts in the introduction section of this paper. kindly refer \there{R3c}.
\BLACK

\item \normalfont{The results are well-structured and clearly presented, but the paper does not fully unpack their implications. The discussion section needs more depth in interpreting the findings: What does it mean for practitioners?} 

\BLUE
Thank you for the follow-up comment. As noted in our response above, the earlier draft did not sufficiently discuss the practical implications of our findings. In addition to those revisions, we have now included a clearer recommendation-for-practitioners section that explains when and where SynthCore should be used. Please refer to \there{R3c}.
\BLACK

Under what conditions might SynthCore not work well?


\BLUE
Thank you for raising this point. SynthCore is not designed to work well in certain settings, and we explicitly acknowledge these limitations in the revised draft refer \there{R3d-b}. In particular, we have not explored SynthCore on ultra–high-dimensional domains such as audio, or image data, where feature spaces can be extremely large and structured.
\BLACK


How should researchers integrate SynthCore into real-world SE pipelines?   For example, could SynthCore reduce the need for human SME input in industry?

\BLUE
Thank you for asking these questions. In response to your query,
we offer a   proposed workflow  \there{R3d-2} and soem general
notes on the impact of SynthCore 
at \there{R3g} 
\BLACK



 
%  Is it feasible to deploy this as part of continuous SE processes?

%  \BLUE
%  We think so, Please our notes on that at  \there{R3d-2} 
 
% \BLACK


%  Overall, the results should be discussed with respect to the nature of high-dimensional optimization tasks. Provide guidance on when to use SynthCore. Discuss the tradeoffs involved in using SynthCore (e.g., does it have increased runtime and LLM costs?) versus simpler prompting approaches.

%   \BLUE
% Thank you for the comment. As addressed in earlier responses, we have included a recommendation diagram indicating when our methods should be used under specific constraints (see \there{R3c}). Our methods are also an order of magnitude faster than state-of-the-art symbolic approaches such as Gaussian Process models (see \ref{runtimes}). Based on these results, we argue that practitioners can use our guidelines to select and adapt faster methods for multi-objective tasks effectively (see \there{R3d}).
% \BLACK


  
\BLACK
\item \normalfont{ Is it feasible to deploy this as part of continuous SE
processes?}

 \BLUE We think so. Please see our notes on that at \there{R3d-2}.
\BLACK
\item \normalfont{ Overall, the results should be discussed with respect to
the nature of high-dimensional optimization tasks. Provide guidance on when to
use SynthCore. Discuss the tradeoffs involved in using SynthCore (e.g., does it
have increased runtime and LLM costs?) versus simpler prompting approaches.}

 \BLUE Thank you for the comment. As addressed in earlier responses, we
have included a recommendation diagram indicating when our methods should be
used under specific constraints (see \there{R3c}). Our methods are also an
order of magnitude faster than state-of-the-art symbolic approaches such as
Gaussian Process models (see \ref{runtimes}). Based on these results, we argue
that practitioners can use our guidelines to select and adapt faster methods
for multi-objective tasks effectively (see \there{R3d}).
\BLACK
\item \normalfont{ Are there scenarios where standard few-shot would be
good enough even in high-dimensional?}

\BLUE Our results suggest not.
\BLACK
\item \normalfont{ Make results more actionable. The paper presents
rankings and performance curves, but readers would also benefit from concrete
takeaways or heuristics (e.g., "if your SE optimization task has more than X
features, use SynthCore with budget $\ge 20$). What do the results imply for
practitioners? For researchers?}

\item \BLUE Thank you for the comment. We agree that the implications of our
results were not presented clearly in the previous draft. In this revision, we
have added explicit recommendations for practitioners, supported by a
recommendation diagram (Figure~\ref{recommend}) that clearly indicates when to
prefer \textit{SynthCore} over standard few-shot learners. The conclusions in
this diagram are grounded in the summarized empirical results reported in
\there{R3d}.
\BLACK
\item \normalfont{ Additional comments for the Introduction section: The
first few lines of the first paragraph need citations.}

 \BLUE Good point.
Now added.
\BLACK 
\item \normalfont{Page 2, line 12: "LLMs appear well-positioned to
streamline SE data collection pipelines" - Would "data labelling be more
fitting in the context of the paper?}

 \BLUE We have now
dropped that sentence.
\BLACK

\item \normalfont{ Page 3, line 5: "see later in this paper" Where? Be
more specific. What section?}

 \BLUE We have now added pointers at that point of the paper
to 
Table~\ref{lo0},~\ref{med0}~\ref{hi0}.
\BLACK
\item \normalfont{ Page 3, line 13: Missing citation.}

 \BLUE We apologize for that, it is rectified.
\BLACK
\item \normalfont{ Page 3, line 32: "this paper explores three research
questions" There are only two research questions.}

 \BLUE We apologize for that, we have corrected in this version to two
research questions .
\BLACK
\item \normalfont{ For each RQ introduced, it is good practice to mention
a brief motivation for it, and present an overview of the results/findings.
RQ2 phrasing assumes the answer to RQ1 is "yes". Consider reframing it so that
it is independent of the previous RQ's outcome.}

 \BLUE Thank you for the suggestion, we have changed the RQs as mentioned.
Please refer \there{R3e}.
\BLACK
\item \normalfont{ Page 4, line 8: "and relatively easy" Based on what?
Either drop the statement or explain why.}

 \BLUE We have decided to drop the statement.
\BLACK
\item \normalfont{ Key concepts introduced too late. Move essential
context to the introduction or early background. Discuss them in more depth in
this section. Use concrete examples to contextualize concepts. Definitions of
terms like "multi-objective optimization" and "active learning" are given, you
give an example in Table 1, but it is still abstract.}

 \BLUE Thank you, your comment is quite correct. We have added
the required material, see \there{R3c}.
\BLACK
\item \normalfont{ Add short explanations to the definitions. For example,
if you are discussing warm-starts, you can add "e.g., pre-selecting
configurations that are likely to perform well when tuning…" Clarify
assumptions and terminology earlier. The distinction between "LLM annotations",
"few-shot learners", and "ensemble learners" becomes clear only deep into the
method sections. Summarize them in a terminology paragraph early.}

 \BLUE Thank you for the suggestion. We have added brief explanations for
the above concepts in the introduction section of this paper. Kindly refer
\there{R3c}.
\BLACK
\item \normalfont{ Additional comments on Literature Review section:
Page 4, line 24: You mention MSR in this case as the concept of "mining
software repositories" but earlier in the paper, it is used as the conference.
You should define it clearly, some readers may not be familiar with this
concept or venue.}

 \BLUE Thank you for pointing this out. We have fixed it

\BLACK
\item \normalfont{ Page 5, line 25: "...this tools three sets of three
hour meetings hours" Needs rephrasing.}
\BLUE
That clumsy section was removed
in our shortenning of the overly-long related work section.
\BLACK
\item \normalfont{ Page 6, line 37: Typo "trainign" $\rightarrow$
training.}

 \BLUE We apologize, we have fixed it.
\BLACK
\item \normalfont{ Page 6, line 46: "The Ahmed et al. study"
$\rightarrow$ Suggestion: Ahmed et al.}

 \BLUE Thank you for the suggestion, we have made the change.
\BLACK
\item \normalfont{ 2.3 Results from Related Work: Very confusing section.
A lot of background concepts are discussed, some parts of the experimental
setup of the paper, comparing studies results. Consider reorganizing.}

\BLUE  We agree the structure was confusing. We have significantly
reorganized that section to separate background concepts from the comparison of
related work results, making the experimental context clearer. 
As for earlier backgrpund, please see \there{R3c}.
\BLACK
\item \normalfont{ Page 9, line 36: "subject matter experts" has already
been defined as SMEs. Use the abbreviations when introducing them.}

 \BLUE Thank you for pointing it out, we have fixed the issue.
\BLACK
\item \normalfont{ Page 21, line 17: Possible typo "the two lower curves
can from simple"}
\BLUE
That sentence was been reorganized and removed.
\BLACK
\item \normalfont{ Page 21, line 45: Possible typo "Tables…show what be
achieved by…"}

 \BLUE Thank you for pointing it out, we have fixed the issue.

 
\BLACK
\item \normalfont{ Are the results presented in this paper directly tied
to the selected model? It seems to me that they would be very dependent on the
model's size, parameters, context size, etc. The justification of the model
selection was very clear, but it makes me wonder how practical the solution is
if it requires the best performing state-of-the-art model to be implemented.
Would you consider repeating your experiment using other models and compare the
results?}

 \BLUE As we say in \there{R0a}
our point is not that LLM1 is better than  LLM2 but rather it is that LLM us better than
symbolic. 
This draft now clarifies the gains are methodological, not just model
dependent.  

\end{enumerate}

\subsection*{Response to Reviewer 4}
\begin{enumerate}[label=\normalfont{Comment \arabic*:}, leftmargin=7.5em]

\item \normalfont{
The reasons for selecting Gemini 1.5 Pro are stated as "a 1-million-token context window and low cost," yet no baseline comparison data with other mainstream models (e.g., GPT-4o) is provided. For example, what are the differences in single-prompt annotation accuracy and runtime between Gemini 1.5 Pro and GPT-4o in low-, medium-, and high-dimensional tasks? The paper only mentions that "gpt-o1 and gemini-1.5pro yield similar results" without providing specific values (such as the mean Chebyshev distance or rank proportion), making it impossible to verify the rationality of the model selection.
}

\BLUE
Thank you for the comment. We apologize for not making our objective clear in the previous draft. Our goal is not to compare or benchmark different LLMs, but rather to compare LLMs with other baseline symbolic methods (UCB, TPE, Bayes Explore/Exploit, etc.). We have clarified this in \there{R2c}; kindly refer to it for details.
\BLACK

\item \normalfont{
The paper notes that the MOOT repository contains 49 tasks, including "configuration, Hyperparameter Optimization (HPO), and process," but fails to clarify the specific optimization objectives and evaluation criteria for each individual task. For instance, datasets like "SS-A" and "SS-B" in Table 2 only label "x/y dimensions" without explaining the actual optimization objectives corresponding to the y-values (e.g., whether it is "minimizing runtime" or "maximizing system throughput"). 
}

\BLUE
Thank you for the comments. While we initially planned to include the complete parameter specifications for all 49 datasets, space constraints made this infeasible. Consequently, we have provided these details as an external reference. kindly refer \there{R4b}
\BLACK 

\item \normalfont{Additionally, the paper does not supplementarily explain what specific configuration parameters the "38 x-variables" in the "SQL AllMeasurements" dataset refer to. This prevents readers from judging the representativeness of the datasets for SE tasks.}

\BLUE 
We appologize for not including these details. However due to space contraints of including 38 varialbe names inside the manuscript we have provided an external reference for these details. kindly refer \there{R4c}.
\BLACK

\item \normalfont{The paper sets L=0.6B (where L is the number of initial randomly annotated samples) and M=20, but does not explain the rationality of these parameter values. For example: Why is L set to 0.6B instead of 0.5B or 0.7B? Is M=20 determined through experimental validation (e.g., performance comparison with M=10 or M=30) or based on empirical settings? }

\BLUE 
We apologize for the earlier lack of clarity. According to the Near Enough Optimization (NEO) framework
(see \there{R4d}) ``near-enough'' solutions are effectively indistinguishable from the true optimum. As shown by the mathematical analysis in \there{R4d}, encountering such a near-optimal solution requires sampling only about 60 times. Our choice of setting $M=20$ follows from the ambition of staying well below this 60-sample NEO threshold. In our experiments, sampling 20 or 30 instances consistently performed as well as sampling any number above this point, with only minimal gains beyond that threshold. Thus, selecting $M=20$ reflects an engineering decision that remains faithful to the NEO theory while keeping the annotation cost low.
\BLACK

\end{enumerate}
\end{document}
% end of file template.tex

